{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rakshith\\Miniconda3\\envs\\machinelearning\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 17722081550549599760\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 6707519816\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 15295732122441999182\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# check for GPU \n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the sun goes down\\nthe stars come out\\nand all that counts\\nis here and now\\nmy universe will never be the same\\ni'm glad you came\\nyou cast a spell on me, spell on me\\nyou hit me like the sky fell on me, fell on me\\nand i decided you look well on me, well on me\\nso let's go somewhere no one else can see, you and me\\nturn the lights out now\\nnow i'll take you by the hand\\nhand you another drink\\ndrink it if you can\\ncan you spend a little time,\\ntime is slipping away,\\naway from us so stay,\\nstay with me i can make,\\nmake you glad you came\\nthe sun goes down\\nsometimes i'm in disbelief i didn't know\\nsomehow i need you to go\\ndon't stay\\nforget our memories\\nforget our possibilities\\nwhat you were changing me into\\n(just give me myself back and)\\ndon't stay\\nforget our memories\\nforget our possibilities\\ntake all your faithlessness with you\\njust give me myself back and\\ndon't stay\\nsometimes i feel like i trusted you too well\\nsometimes i just feel like screaming at myself\\nsometimes i'm in disbelief i didn't know\\nsomehow i need to be alone\\ndon't stay\\nforget our memories\\nforget our possibilities\\nwhat you were changing me into\\n(just give me myself back and)\\ndon't stay\\nforget our memories\\nforget our possibilities\\ntake all your faithlessness with you\\njust give me myself back and\\ndon't stay\\ni don't need you anymore, i don't want to be ignored\\ni don't need one more day of you wasting me away\\ni don't need you anymore, i don't want to be ignored\\ni don't need one more day of you wasting me away\\nwith no apologies\\ndon't stay\\nforget our memories\\nforget our possibilities\\nwhat you were changing me into\\n(just give me myself back and)\\ndon't stay\\nforget our memories\\nforget our possibilities\\ntake all your faithlessness with you\\njust give me myself back and\\ndon't stay\\nthe stars come out\\nand all that counts\\nis here and now\\nmy universe will never be the same\\ni'm glad you came\\ni'm glad you came\\nyou cast a spell on me, spell on me\\nyou hit me like the sky fell on me, fell on me\\nand i decided you look well on me, well on me\\nso let's go somewhere no one else can see, you and me\\nturn the lights out now\\nnow i'll take you by the hand\\nhand you another drink\\ndrink it if you can\\ncan you spend a little time,\\ntime is slipping away,\\naway from us so stay,\\nstay with me i can make,\\nmake you glad you came\\nthe sun goes down\\nthe stars come out\\nand all that counts\\nis here and now\\nmy universe will never be the same\\ni'm glad you came\\ni'm glad you came\\ni'm glad you came\\nso glad you came\\ni'm glad you came\\ni'm glad you came\\nthe sun goes down\\nthe stars come out\\nand all that counts\\nis here and now\\nmy universe will never be the same\\ni'm glad you came\\ni'm glad you came\\nthere are just too many times that people have tried to look inside of me\\nwondering what i think of you and i protect you out of courtesy\\ntoo many times that i've held on when i needed to push away\\nafraid to say what was on my mind, afraid to say what i need to say\\ntoo many things that you've said about me when i'm not around\\nyou think having the upper hand means you gotta keep putting me down\\nbut i've had too many stand-offs with you, it's about as much as i can stand\\nso i'm waiting until the upper hand is mine\\nthe next you're not, watch it drop\\n(making your heart stop)\\njust before you hit the floor\\n(one minute you're on top)\\nthe next you're not, missed your shot\\n(making your heart stop)\\nyou think you won\\n(and then it's all gone)\\nso many people like me put so much trust in all your lies\\nso concerned with what you think to just say what we feel inside\\nso many people like me walk on eggshells all day long\\nall i know is that all i want is to feel like i'm not stepped on\\nthere are so many things you say that make me feel you've crossed the line\\nwhat goes up will surely fall and i'm counting down the time\\n'cause i've had so many stand-offs with you, it's about as much as i can stand\\nso i'm waiting until the upper hand is mine\\n(one minute you're on top)\\nthe next you're not, watch it drop\\n(making your heart stop)\\njust before you hit the floor\\n(one minute you're on top)\\nthe next you're not, missed your shot\\n(making your heart stop)\\nyou think you won\\n(and then it's all gone)\\n(now it's all gone)\\ni know i'll never trust a single thing you say\\nyou knew your lies would divide us, but you lied anyway\\nand all the lies have got you floating up above us all\\nbut what goes up has got to fall\\n(one minute you're on top)\\nthe next you're not, watch it drop\\n(making your heart stop)\\njust before you hit the floor\\n(one minute you're on top)\\nthe next you're not, missed your shot\\n(making your heart stop)\\nyou think you won\\n(and then it's all gone)\\n(now it's all gone)\\ni'm tired of being what you want me to be\\nfeeling so faithless, lost under the surface\\ndon't know what you're expecting of me\\nput under the pressure of walking in your shoes\\n(caught in the undertow, just caught in the undertow)\\nevery step that i take is another mistake to you\\n(caught in the undertow, just caught in the undertow)\\ni've become so numb, i can't feel you there\\nbecome so tired, so much more aware\\ni'm becoming this, all i want to do\\nis be more like me and be less like you\\ncan't you see that you're smothering me,\\nholding too tightly, afraid to lose control?\\n'cause everything that you thought i would be\\nhas fallen apart right in front of you.\\n(caught in the undertow, just caught in the undertow)\\nevery step that i take is another mistake to you.\\n(caught in the undertow, just caught in the undertow)\\nand every second i waste is more than i can take.\\ni've become so numb, i can't feel you there,\\nbecome so tired, so much more aware\\ni'm becoming this, all i want to do\\nis be more like me and be less like you. and i know\\ni may end up failing too.\\nbut i know\\nyou were just like me with someone disappointed in you.\\ni've become so numb, i can't feel you there,\\nbecome so tired, so much more aware.\\ni'm becoming this, all i want to do\\nis be more like me and be less like you.\\ni've become so numb, i can't feel you there.\\n(i'm tired of being what you want me to be)\\ni've become so numb, i can't feel you there.\\n(i'm tired of being what you want me to be)\\ni dreamed i was missing\\nyou were so scared\\nbut no one would listen\\n'cause no one else cared\\nafter my dreaming\\ni woke with this fear\\nwhat am i leaving\\nwhen i'm done here?\\nso if you're asking me\\ni want you to know\\nwhen my time comes\\nforget the wrong that i've done\\nhelp me leave behind some\\nreasons to be missed\\nand don't resent me\\nand when you're feeling empty\\nkeep me in your memory\\nleave out all the rest\\nleave out all the rest\\ndon't be afraid\\ni've taken my beating\\ni've shared what i've made\\ni'm strong on the surface\\nnot all the way through\\ni've never been perfect\\nbut neither have you\\nso if you're asking me\\ni want you to know\\nwhen my time comes\\nforget the wrong that i've done\\nhelp me leave behind some\\nreasons to be missed\\ndon't resent me\\nand when you're feeling empty\\nkeep me in your memory\\nleave out all the rest\\nleave out all the rest\\nforgetting all the hurt inside\\nyou've learned to hide so well\\npretending someone else can come\\nand save me from myself\\ni can't be who you are\\nwhen my time comes\\nforget the wrong that i've done\\nhelp me leave behind some\\nreasons to be missed\\ndon't resent me\\nand when you're feeling empty\\nkeep me in your memory\\nleave out all the rest\\nleave out all the rest\\nforgetting all the hurt inside\\nyou've learned to hide so well\\npretending someone else can come\\nand save me from myself\\ni can't be who you are\\ni can't be who you are\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "filename = \"dataset/lovelyrics.txt\"\n",
    "raw_text = open(filename,encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()\n",
    "raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get all the chars in the sequence of text\n",
    "\n",
    "chars = sorted(list(set(raw_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert to dicts \n",
    "\n",
    "char_to_int = {ch:i for i,ch in enumerate(chars)}\n",
    "int_to_char = {i:ch for i,ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters: 7437 \n",
      "Total Vocab: 33\n"
     ]
    }
   ],
   "source": [
    "# setup the variables used later \n",
    "\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print (\"Total Characters: {} \".format(n_chars))\n",
    "print (\"Total Vocab: {}\".format(n_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  7337\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16,\n",
       " 13,\n",
       " 1,\n",
       " 26,\n",
       " 9,\n",
       " 21,\n",
       " 13,\n",
       " 0,\n",
       " 17,\n",
       " 2,\n",
       " 21,\n",
       " 1,\n",
       " 15,\n",
       " 20,\n",
       " 9,\n",
       " 12,\n",
       " 1,\n",
       " 32,\n",
       " 23,\n",
       " 28,\n",
       " 1,\n",
       " 11,\n",
       " 9,\n",
       " 21,\n",
       " 13,\n",
       " 0,\n",
       " 32,\n",
       " 23,\n",
       " 28,\n",
       " 1,\n",
       " 11,\n",
       " 9,\n",
       " 26,\n",
       " 27,\n",
       " 1,\n",
       " 9,\n",
       " 1,\n",
       " 26,\n",
       " 24,\n",
       " 13,\n",
       " 20,\n",
       " 20,\n",
       " 1,\n",
       " 23,\n",
       " 22,\n",
       " 1,\n",
       " 21,\n",
       " 13,\n",
       " 5,\n",
       " 1,\n",
       " 26,\n",
       " 24,\n",
       " 13,\n",
       " 20,\n",
       " 20,\n",
       " 1,\n",
       " 23,\n",
       " 22,\n",
       " 1,\n",
       " 21,\n",
       " 13,\n",
       " 0,\n",
       " 32,\n",
       " 23,\n",
       " 28,\n",
       " 1,\n",
       " 16,\n",
       " 17,\n",
       " 27,\n",
       " 1,\n",
       " 21,\n",
       " 13,\n",
       " 1,\n",
       " 20,\n",
       " 17,\n",
       " 19,\n",
       " 13,\n",
       " 1,\n",
       " 27,\n",
       " 16,\n",
       " 13,\n",
       " 1,\n",
       " 26,\n",
       " 19,\n",
       " 32,\n",
       " 1,\n",
       " 14,\n",
       " 13,\n",
       " 20,\n",
       " 20,\n",
       " 1,\n",
       " 23,\n",
       " 22,\n",
       " 1,\n",
       " 21,\n",
       " 13,\n",
       " 5,\n",
       " 1,\n",
       " 14,\n",
       " 13,\n",
       " 20,\n",
       " 20,\n",
       " 1,\n",
       " 23,\n",
       " 22,\n",
       " 1,\n",
       " 21,\n",
       " 13,\n",
       " 0,\n",
       " 9,\n",
       " 22,\n",
       " 12,\n",
       " 1,\n",
       " 17,\n",
       " 1,\n",
       " 12,\n",
       " 13,\n",
       " 11,\n",
       " 17,\n",
       " 12,\n",
       " 13,\n",
       " 12,\n",
       " 1,\n",
       " 32,\n",
       " 23,\n",
       " 28,\n",
       " 1,\n",
       " 20,\n",
       " 23,\n",
       " 23,\n",
       " 19,\n",
       " 1,\n",
       " 30,\n",
       " 13,\n",
       " 20,\n",
       " 20,\n",
       " 1,\n",
       " 23,\n",
       " 22,\n",
       " 1,\n",
       " 21,\n",
       " 13,\n",
       " 5,\n",
       " 1,\n",
       " 30,\n",
       " 13,\n",
       " 20,\n",
       " 20,\n",
       " 1,\n",
       " 23,\n",
       " 22,\n",
       " 1,\n",
       " 21,\n",
       " 13,\n",
       " 0,\n",
       " 26,\n",
       " 23,\n",
       " 1,\n",
       " 20,\n",
       " 13,\n",
       " 27,\n",
       " 2,\n",
       " 26,\n",
       " 1,\n",
       " 15,\n",
       " 23,\n",
       " 1,\n",
       " 26,\n",
       " 23,\n",
       " 21,\n",
       " 13,\n",
       " 30,\n",
       " 16,\n",
       " 13,\n",
       " 25,\n",
       " 13,\n",
       " 1,\n",
       " 22,\n",
       " 23,\n",
       " 1,\n",
       " 23,\n",
       " 22,\n",
       " 13,\n",
       " 1,\n",
       " 13,\n",
       " 20,\n",
       " 26,\n",
       " 13,\n",
       " 1,\n",
       " 11,\n",
       " 9,\n",
       " 22,\n",
       " 1,\n",
       " 26,\n",
       " 13,\n",
       " 13,\n",
       " 5,\n",
       " 1,\n",
       " 32,\n",
       " 23,\n",
       " 28,\n",
       " 1,\n",
       " 9,\n",
       " 22,\n",
       " 12,\n",
       " 1,\n",
       " 21,\n",
       " 13,\n",
       " 0,\n",
       " 27,\n",
       " 28,\n",
       " 25,\n",
       " 22,\n",
       " 1,\n",
       " 27,\n",
       " 16,\n",
       " 13,\n",
       " 1,\n",
       " 20,\n",
       " 17,\n",
       " 15,\n",
       " 16,\n",
       " 27,\n",
       " 26,\n",
       " 1,\n",
       " 23,\n",
       " 28,\n",
       " 27,\n",
       " 1,\n",
       " 22,\n",
       " 23,\n",
       " 30,\n",
       " 0,\n",
       " 22,\n",
       " 23,\n",
       " 30,\n",
       " 1,\n",
       " 17,\n",
       " 2,\n",
       " 20,\n",
       " 20,\n",
       " 1,\n",
       " 27,\n",
       " 9,\n",
       " 19,\n",
       " 13,\n",
       " 1,\n",
       " 32,\n",
       " 23,\n",
       " 28,\n",
       " 1,\n",
       " 10,\n",
       " 32,\n",
       " 1,\n",
       " 27,\n",
       " 16,\n",
       " 13,\n",
       " 1,\n",
       " 16,\n",
       " 9,\n",
       " 22,\n",
       " 12,\n",
       " 0,\n",
       " 16,\n",
       " 9,\n",
       " 22,\n",
       " 12,\n",
       " 1,\n",
       " 32,\n",
       " 23,\n",
       " 28,\n",
       " 1,\n",
       " 9,\n",
       " 22,\n",
       " 23,\n",
       " 27,\n",
       " 16,\n",
       " 13,\n",
       " 25,\n",
       " 1,\n",
       " 12,\n",
       " 25,\n",
       " 17,\n",
       " 22,\n",
       " 19,\n",
       " 0,\n",
       " 12,\n",
       " 25,\n",
       " 17,\n",
       " 22,\n",
       " 19,\n",
       " 1,\n",
       " 17,\n",
       " 27,\n",
       " 1,\n",
       " 17,\n",
       " 14,\n",
       " 1,\n",
       " 32,\n",
       " 23,\n",
       " 28,\n",
       " 1,\n",
       " 11,\n",
       " 9,\n",
       " 22,\n",
       " 0,\n",
       " 11,\n",
       " 9,\n",
       " 22,\n",
       " 1,\n",
       " 32,\n",
       " 23,\n",
       " 28,\n",
       " 1,\n",
       " 26,\n",
       " 24,\n",
       " 13,\n",
       " 22,\n",
       " 12,\n",
       " 1,\n",
       " 9,\n",
       " 1,\n",
       " 20,\n",
       " 17,\n",
       " 27,\n",
       " 27,\n",
       " 20,\n",
       " 13,\n",
       " 1,\n",
       " 27,\n",
       " 17,\n",
       " 21,\n",
       " 13,\n",
       " 5,\n",
       " 0,\n",
       " 27,\n",
       " 17,\n",
       " 21,\n",
       " 13,\n",
       " 1,\n",
       " 17,\n",
       " 26,\n",
       " 1,\n",
       " 26,\n",
       " 20,\n",
       " 17,\n",
       " 24,\n",
       " 24,\n",
       " 17,\n",
       " 22,\n",
       " 15,\n",
       " 1,\n",
       " 9,\n",
       " 30,\n",
       " 9,\n",
       " 32,\n",
       " 5,\n",
       " 0,\n",
       " 9,\n",
       " 30,\n",
       " 9,\n",
       " 32,\n",
       " 1,\n",
       " 14,\n",
       " 25,\n",
       " 23,\n",
       " 21,\n",
       " 1,\n",
       " 28,\n",
       " 26,\n",
       " 1,\n",
       " 26,\n",
       " 23,\n",
       " 1,\n",
       " 26,\n",
       " 27,\n",
       " 9,\n",
       " 32,\n",
       " 5,\n",
       " 0,\n",
       " 26,\n",
       " 27,\n",
       " 9,\n",
       " 32,\n",
       " 1,\n",
       " 30,\n",
       " 17,\n",
       " 27,\n",
       " 16,\n",
       " 1,\n",
       " 21,\n",
       " 13,\n",
       " 1,\n",
       " 17,\n",
       " 1,\n",
       " 11,\n",
       " 9,\n",
       " 22,\n",
       " 1,\n",
       " 21,\n",
       " 9,\n",
       " 19,\n",
       " 13,\n",
       " 5,\n",
       " 0,\n",
       " 21,\n",
       " 9,\n",
       " 19,\n",
       " 13,\n",
       " 1,\n",
       " 32,\n",
       " 23,\n",
       " 28,\n",
       " 1,\n",
       " 15,\n",
       " 20,\n",
       " 9,\n",
       " 12,\n",
       " 1,\n",
       " 32,\n",
       " 23,\n",
       " 28,\n",
       " 1,\n",
       " 11,\n",
       " 9,\n",
       " 21,\n",
       " 13,\n",
       " 0,\n",
       " 27,\n",
       " 16,\n",
       " 13,\n",
       " 1,\n",
       " 26,\n",
       " 28,\n",
       " 22,\n",
       " 1,\n",
       " 15,\n",
       " 23,\n",
       " 13,\n",
       " 26,\n",
       " 1,\n",
       " 12,\n",
       " 23,\n",
       " 30,\n",
       " 22,\n",
       " 0,\n",
       " 26,\n",
       " 23,\n",
       " 21,\n",
       " 13,\n",
       " 27,\n",
       " 17,\n",
       " 21,\n",
       " 13,\n",
       " 26,\n",
       " 1,\n",
       " 17,\n",
       " 2,\n",
       " 21,\n",
       " 1,\n",
       " 17,\n",
       " 22,\n",
       " 1,\n",
       " 12,\n",
       " 17,\n",
       " 26,\n",
       " 10,\n",
       " 13,\n",
       " 20,\n",
       " 17,\n",
       " 13,\n",
       " 14,\n",
       " 1,\n",
       " 17,\n",
       " 1,\n",
       " 12,\n",
       " 17,\n",
       " 12,\n",
       " 22,\n",
       " 2,\n",
       " 27,\n",
       " 1,\n",
       " 19,\n",
       " 22,\n",
       " 23,\n",
       " 30,\n",
       " 0,\n",
       " 26,\n",
       " 23,\n",
       " 21,\n",
       " 13,\n",
       " 16,\n",
       " 23,\n",
       " 30,\n",
       " 1,\n",
       " 17,\n",
       " 1,\n",
       " 22,\n",
       " 13,\n",
       " 13,\n",
       " 12,\n",
       " 1,\n",
       " 32,\n",
       " 23,\n",
       " 28,\n",
       " 1,\n",
       " 27,\n",
       " 23,\n",
       " 1,\n",
       " 15,\n",
       " 23,\n",
       " 0,\n",
       " 12,\n",
       " 23,\n",
       " 22,\n",
       " 2,\n",
       " 27,\n",
       " 1,\n",
       " 26,\n",
       " 27,\n",
       " 9,\n",
       " 32,\n",
       " 0,\n",
       " 14,\n",
       " 23,\n",
       " 25,\n",
       " 15,\n",
       " 13,\n",
       " 27,\n",
       " 1,\n",
       " 23,\n",
       " 28,\n",
       " 25,\n",
       " 1,\n",
       " 21,\n",
       " 13,\n",
       " 21,\n",
       " 23,\n",
       " 25,\n",
       " 17,\n",
       " 13,\n",
       " 26,\n",
       " 0,\n",
       " 14,\n",
       " 23,\n",
       " 25,\n",
       " 15,\n",
       " 13,\n",
       " 27,\n",
       " 1,\n",
       " 23,\n",
       " 28,\n",
       " 25,\n",
       " 1,\n",
       " 24,\n",
       " 23,\n",
       " 26,\n",
       " 26,\n",
       " 17,\n",
       " 10,\n",
       " 17,\n",
       " 20,\n",
       " 17,\n",
       " 27,\n",
       " 17,\n",
       " 13,\n",
       " 26,\n",
       " 0,\n",
       " 30,\n",
       " 16,\n",
       " 9,\n",
       " 27,\n",
       " 1,\n",
       " 32,\n",
       " 23,\n",
       " 28,\n",
       " 1,\n",
       " 30,\n",
       " 13,\n",
       " 25,\n",
       " 13,\n",
       " 1,\n",
       " 11,\n",
       " 16,\n",
       " 9,\n",
       " 22,\n",
       " 15,\n",
       " 17,\n",
       " 22,\n",
       " 15,\n",
       " 1,\n",
       " 21,\n",
       " 13,\n",
       " 1,\n",
       " 17,\n",
       " 22,\n",
       " 27,\n",
       " 23,\n",
       " 0,\n",
       " 3,\n",
       " 18,\n",
       " 28,\n",
       " 26,\n",
       " 27,\n",
       " 1,\n",
       " 15,\n",
       " 17,\n",
       " 29,\n",
       " 13,\n",
       " 1,\n",
       " 21,\n",
       " 13,\n",
       " 1,\n",
       " 21,\n",
       " 32,\n",
       " 26,\n",
       " 13,\n",
       " 20,\n",
       " 14,\n",
       " 1,\n",
       " 10,\n",
       " 9,\n",
       " 11,\n",
       " 19,\n",
       " 1,\n",
       " 9,\n",
       " 22,\n",
       " 12,\n",
       " 4,\n",
       " 0,\n",
       " 12,\n",
       " 23,\n",
       " 22,\n",
       " 2,\n",
       " 27,\n",
       " 1,\n",
       " 26,\n",
       " 27,\n",
       " 9,\n",
       " 32,\n",
       " 0,\n",
       " 14,\n",
       " 23,\n",
       " 25,\n",
       " 15,\n",
       " 13,\n",
       " 27,\n",
       " 1,\n",
       " 23,\n",
       " 28,\n",
       " 25,\n",
       " 1,\n",
       " 21,\n",
       " 13,\n",
       " 21,\n",
       " 23,\n",
       " 25,\n",
       " 17,\n",
       " 13,\n",
       " 26,\n",
       " 0,\n",
       " 14,\n",
       " 23,\n",
       " 25,\n",
       " 15,\n",
       " 13,\n",
       " 27,\n",
       " 1,\n",
       " 23,\n",
       " 28,\n",
       " 25,\n",
       " 1,\n",
       " 24,\n",
       " 23,\n",
       " 26,\n",
       " 26,\n",
       " 17,\n",
       " 10,\n",
       " 17,\n",
       " 20,\n",
       " 17,\n",
       " 27,\n",
       " 17,\n",
       " 13,\n",
       " 26,\n",
       " 0,\n",
       " 27,\n",
       " 9,\n",
       " 19,\n",
       " 13,\n",
       " 1,\n",
       " 9,\n",
       " 20,\n",
       " 20,\n",
       " 1,\n",
       " 32,\n",
       " 23,\n",
       " 28,\n",
       " 25,\n",
       " 1,\n",
       " 14,\n",
       " 9,\n",
       " 17,\n",
       " 27,\n",
       " 16,\n",
       " 20,\n",
       " 13,\n",
       " 26,\n",
       " 26,\n",
       " 22,\n",
       " 13,\n",
       " 26,\n",
       " 26,\n",
       " 1,\n",
       " 30,\n",
       " 17,\n",
       " 27,\n",
       " 16,\n",
       " 1,\n",
       " 32,\n",
       " 23,\n",
       " 28,\n",
       " 0,\n",
       " 18,\n",
       " 28,\n",
       " 26,\n",
       " 27,\n",
       " 1,\n",
       " 15,\n",
       " 17,\n",
       " 29,\n",
       " 13,\n",
       " 1,\n",
       " 21,\n",
       " 13,\n",
       " 1,\n",
       " 21,\n",
       " 32,\n",
       " 26,\n",
       " 13,\n",
       " 20,\n",
       " 14,\n",
       " 1,\n",
       " 10,\n",
       " 9,\n",
       " 11,\n",
       " 19,\n",
       " 1,\n",
       " 9,\n",
       " 22,\n",
       " 12,\n",
       " 0,\n",
       " 12,\n",
       " 23,\n",
       " 22,\n",
       " 2,\n",
       " 27,\n",
       " 1,\n",
       " 26,\n",
       " 27,\n",
       " 9,\n",
       " 32,\n",
       " 0,\n",
       " 26,\n",
       " 23,\n",
       " 21,\n",
       " 13,\n",
       " 27,\n",
       " 17,\n",
       " 21,\n",
       " 13,\n",
       " 26,\n",
       " 1,\n",
       " 17,\n",
       " 1,\n",
       " 14,\n",
       " 13,\n",
       " 13,\n",
       " 20,\n",
       " 1,\n",
       " 20,\n",
       " 17,\n",
       " 19,\n",
       " 13,\n",
       " 1,\n",
       " 17,\n",
       " 1,\n",
       " 27,\n",
       " 25,\n",
       " 28,\n",
       " 26,\n",
       " 27,\n",
       " 13,\n",
       " 12,\n",
       " 1,\n",
       " 32,\n",
       " 23,\n",
       " 28,\n",
       " 1,\n",
       " 27,\n",
       " 23,\n",
       " 23,\n",
       " 1,\n",
       " 30,\n",
       " 13,\n",
       " 20,\n",
       " 20,\n",
       " 0,\n",
       " 26,\n",
       " 23,\n",
       " 21,\n",
       " 13,\n",
       " 27,\n",
       " 17,\n",
       " 21,\n",
       " 13,\n",
       " 26,\n",
       " 1,\n",
       " 17,\n",
       " 1,\n",
       " 18,\n",
       " 28,\n",
       " 26,\n",
       " 27,\n",
       " 1,\n",
       " 14,\n",
       " 13,\n",
       " 13,\n",
       " 20,\n",
       " 1,\n",
       " 20,\n",
       " 17,\n",
       " 19,\n",
       " 13,\n",
       " 1,\n",
       " 26,\n",
       " 11,\n",
       " 25,\n",
       " 13,\n",
       " 9,\n",
       " 21,\n",
       " 17,\n",
       " 22,\n",
       " 15,\n",
       " 1,\n",
       " 9,\n",
       " 27,\n",
       " 1,\n",
       " 21,\n",
       " 32,\n",
       " 26,\n",
       " 13,\n",
       " 20,\n",
       " 14,\n",
       " 0,\n",
       " 26,\n",
       " 23,\n",
       " 21,\n",
       " 13,\n",
       " 27,\n",
       " 17,\n",
       " 21,\n",
       " 13,\n",
       " 26,\n",
       " 1,\n",
       " 17,\n",
       " 2,\n",
       " 21,\n",
       " 1,\n",
       " 17,\n",
       " 22,\n",
       " 1,\n",
       " 12,\n",
       " 17,\n",
       " 26,\n",
       " 10,\n",
       " 13,\n",
       " 20,\n",
       " 17,\n",
       " 13,\n",
       " 14,\n",
       " 1,\n",
       " 17,\n",
       " 1,\n",
       " 12,\n",
       " 17,\n",
       " 12,\n",
       " 22,\n",
       " 2,\n",
       " 27,\n",
       " 1,\n",
       " 19,\n",
       " 22,\n",
       " 23,\n",
       " 30,\n",
       " 0,\n",
       " 26,\n",
       " 23,\n",
       " 21,\n",
       " 13,\n",
       " 16,\n",
       " 23,\n",
       " 30,\n",
       " 1,\n",
       " 17,\n",
       " 1,\n",
       " 22,\n",
       " 13,\n",
       " 13,\n",
       " 12,\n",
       " 1,\n",
       " 27,\n",
       " 23,\n",
       " 1,\n",
       " 10,\n",
       " 13,\n",
       " 1,\n",
       " 9,\n",
       " 20,\n",
       " 23,\n",
       " 22,\n",
       " 13,\n",
       " 0,\n",
       " 12,\n",
       " 23,\n",
       " 22,\n",
       " 2,\n",
       " 27,\n",
       " 1,\n",
       " 26,\n",
       " 27,\n",
       " 9,\n",
       " 32,\n",
       " 0,\n",
       " 14,\n",
       " 23,\n",
       " 25,\n",
       " 15,\n",
       " 13,\n",
       " 27,\n",
       " 1,\n",
       " 23,\n",
       " 28,\n",
       " 25,\n",
       " 1,\n",
       " 21,\n",
       " 13,\n",
       " 21,\n",
       " 23,\n",
       " 25,\n",
       " 17,\n",
       " 13,\n",
       " 26,\n",
       " 0,\n",
       " 14,\n",
       " 23,\n",
       " 25,\n",
       " 15,\n",
       " 13,\n",
       " 27,\n",
       " 1,\n",
       " 23,\n",
       " 28,\n",
       " 25,\n",
       " 1,\n",
       " 24,\n",
       " 23,\n",
       " 26,\n",
       " 26,\n",
       " 17,\n",
       " 10,\n",
       " 17,\n",
       " 20,\n",
       " 17,\n",
       " 27,\n",
       " 17,\n",
       " 13,\n",
       " 26,\n",
       " 0,\n",
       " 30,\n",
       " 16,\n",
       " 9,\n",
       " 27,\n",
       " 1,\n",
       " 32,\n",
       " 23,\n",
       " 28,\n",
       " 1,\n",
       " 30,\n",
       " 13,\n",
       " 25,\n",
       " 13,\n",
       " 1,\n",
       " 11,\n",
       " 16,\n",
       " 9,\n",
       " 22,\n",
       " 15,\n",
       " 17,\n",
       " 22,\n",
       " ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Rakshith\\Miniconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1192: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From C:\\Users\\Rakshith\\Miniconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1299: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "# define the LSTM model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "\n",
    "filepath=\"love-songs-weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the model from previous epoch - 1ooth epoch\n",
    "# load the network weights\n",
    "# filename = \"eminem-weights-improvement-13-2.2727.hdf5\"\n",
    "# model.load_weights(filename)\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "7296/7337 [============================>.] - ETA: 0s - loss: 3.0819Epoch 00000: loss improved from inf to 3.08019, saving model to love-songs-weights-improvement-00-3.0802.hdf5\n",
      "7337/7337 [==============================] - 34s - loss: 3.0802    \n",
      "Epoch 2/500\n",
      "7296/7337 [============================>.] - ETA: 0s - loss: 3.0057Epoch 00001: loss improved from 3.08019 to 3.00599, saving model to love-songs-weights-improvement-01-3.0060.hdf5\n",
      "7337/7337 [==============================] - 19s - loss: 3.0060    \n",
      "Epoch 3/500\n",
      "7296/7337 [============================>.] - ETA: 0s - loss: 3.0054Epoch 00002: loss improved from 3.00599 to 3.00474, saving model to love-songs-weights-improvement-02-3.0047.hdf5\n",
      "7337/7337 [==============================] - 13s - loss: 3.0047    \n",
      "Epoch 4/500\n",
      "7296/7337 [============================>.] - ETA: 0s - loss: 2.9970Epoch 00003: loss improved from 3.00474 to 2.99751, saving model to love-songs-weights-improvement-03-2.9975.hdf5\n",
      "7337/7337 [==============================] - 13s - loss: 2.9975    \n",
      "Epoch 5/500\n",
      "7296/7337 [============================>.] - ETA: 0s - loss: 2.9846Epoch 00004: loss improved from 2.99751 to 2.98439, saving model to love-songs-weights-improvement-04-2.9844.hdf5\n",
      "7337/7337 [==============================] - 13s - loss: 2.9844    \n",
      "Epoch 6/500\n",
      "7296/7337 [============================>.] - ETA: 0s - loss: 2.9413Epoch 00005: loss improved from 2.98439 to 2.94050, saving model to love-songs-weights-improvement-05-2.9405.hdf5\n",
      "7337/7337 [==============================] - 13s - loss: 2.9405    \n",
      "Epoch 7/500\n",
      "7296/7337 [============================>.] - ETA: 0s - loss: 2.8615Epoch 00006: loss improved from 2.94050 to 2.86216, saving model to love-songs-weights-improvement-06-2.8622.hdf5\n",
      "7337/7337 [==============================] - 13s - loss: 2.8622    \n",
      "Epoch 8/500\n",
      "7296/7337 [============================>.] - ETA: 0s - loss: 2.7871Epoch 00007: loss improved from 2.86216 to 2.78738, saving model to love-songs-weights-improvement-07-2.7874.hdf5\n",
      "7337/7337 [==============================] - 13s - loss: 2.7874    \n",
      "Epoch 9/500\n",
      "7296/7337 [============================>.] - ETA: 0s - loss: 2.7230Epoch 00008: loss improved from 2.78738 to 2.72280, saving model to love-songs-weights-improvement-08-2.7228.hdf5\n",
      "7337/7337 [==============================] - 13s - loss: 2.7228    \n",
      "Epoch 10/500\n",
      "7296/7337 [============================>.] - ETA: 0s - loss: 2.6717Epoch 00009: loss improved from 2.72280 to 2.67160, saving model to love-songs-weights-improvement-09-2.6716.hdf5\n",
      "7337/7337 [==============================] - 13s - loss: 2.6716    \n",
      "Epoch 11/500\n",
      "7296/7337 [============================>.] - ETA: 0s - loss: 2.6213Epoch 00010: loss improved from 2.67160 to 2.62168, saving model to love-songs-weights-improvement-10-2.6217.hdf5\n",
      "7337/7337 [==============================] - 13s - loss: 2.6217    \n",
      "Epoch 12/500\n",
      "7296/7337 [============================>.] - ETA: 0s - loss: 2.5724Epoch 00011: loss improved from 2.62168 to 2.57150, saving model to love-songs-weights-improvement-11-2.5715.hdf5\n",
      "7337/7337 [==============================] - 13s - loss: 2.5715    \n",
      "Epoch 13/500\n",
      "7296/7337 [============================>.] - ETA: 0s - loss: 2.5151Epoch 00012: loss improved from 2.57150 to 2.51621, saving model to love-songs-weights-improvement-12-2.5162.hdf5\n",
      "7337/7337 [==============================] - 13s - loss: 2.5162    \n",
      "Epoch 14/500\n",
      "7296/7337 [============================>.] - ETA: 0s - loss: 2.4609Epoch 00013: loss improved from 2.51621 to 2.46045, saving model to love-songs-weights-improvement-13-2.4605.hdf5\n",
      "7337/7337 [==============================] - 13s - loss: 2.4605    \n",
      "Epoch 15/500\n",
      "7296/7337 [============================>.] - ETA: 0s - loss: 2.3843Epoch 00014: loss improved from 2.46045 to 2.38456, saving model to love-songs-weights-improvement-14-2.3846.hdf5\n",
      "7337/7337 [==============================] - 13s - loss: 2.3846    \n",
      "Epoch 16/500\n",
      "7296/7337 [============================>.] - ETA: 0s - loss: 2.3011Epoch 00015: loss improved from 2.38456 to 2.30199, saving model to love-songs-weights-improvement-15-2.3020.hdf5\n",
      "7337/7337 [==============================] - 13s - loss: 2.3020    \n",
      "Epoch 17/500\n",
      "7296/7337 [============================>.] - ETA: 0s - loss: 2.1971Epoch 00016: loss improved from 2.30199 to 2.19645, saving model to love-songs-weights-improvement-16-2.1964.hdf5\n",
      "7337/7337 [==============================] - 13s - loss: 2.1964    \n",
      "Epoch 18/500\n",
      "7296/7337 [============================>.] - ETA: 0s - loss: 2.0866Epoch 00017: loss improved from 2.19645 to 2.08583, saving model to love-songs-weights-improvement-17-2.0858.hdf5\n",
      "7337/7337 [==============================] - 13s - loss: 2.0858    \n",
      "Epoch 19/500\n",
      "7296/7337 [============================>.] - ETA: 0s - loss: 1.9473Epoch 00018: loss improved from 2.08583 to 1.94955, saving model to love-songs-weights-improvement-18-1.9495.hdf5\n",
      "7337/7337 [==============================] - 13s - loss: 1.9495    \n",
      "Epoch 20/500\n",
      "7296/7337 [============================>.] - ETA: 0s - loss: 1.8032Epoch 00019: loss improved from 1.94955 to 1.80263, saving model to love-songs-weights-improvement-19-1.8026.hdf5\n",
      "7337/7337 [==============================] - 13s - loss: 1.8026    \n",
      "Epoch 21/500\n",
      "7296/7337 [============================>.] - ETA: 0s - loss: 1.6545Epoch 00020: loss improved from 1.80263 to 1.65436, saving model to love-songs-weights-improvement-20-1.6544.hdf5\n",
      "7337/7337 [==============================] - 13s - loss: 1.6544    \n",
      "Epoch 22/500\n",
      "7296/7337 [============================>.] - ETA: 0s - loss: 1.5187Epoch 00021: loss improved from 1.65436 to 1.51928, saving model to love-songs-weights-improvement-21-1.5193.hdf5\n",
      "7337/7337 [==============================] - 13s - loss: 1.5193    \n",
      "Epoch 23/500\n",
      "7296/7337 [============================>.] - ETA: 0s - loss: 1.3719Epoch 00022: loss improved from 1.51928 to 1.37397, saving model to love-songs-weights-improvement-22-1.3740.hdf5\n",
      "7337/7337 [==============================] - 13s - loss: 1.3740    \n",
      "Epoch 24/500\n",
      "7296/7337 [============================>.] - ETA: 0s - loss: 1.2438Epoch 00023: loss improved from 1.37397 to 1.24403, saving model to love-songs-weights-improvement-23-1.2440.hdf5\n",
      "7337/7337 [==============================] - 13s - loss: 1.2440    \n",
      "Epoch 25/500\n",
      "7296/7337 [============================>.] - ETA: 0s - loss: 1.1273Epoch 00024: loss improved from 1.24403 to 1.12677, saving model to love-songs-weights-improvement-24-1.1268.hdf5\n",
      "7337/7337 [==============================] - 13s - loss: 1.1268    \n",
      "Epoch 26/500\n",
      "7296/7337 [============================>.] - ETA: 0s - loss: 1.0131Epoch 00025: loss improved from 1.12677 to 1.01344, saving model to love-songs-weights-improvement-25-1.0134.hdf5\n",
      "7337/7337 [==============================] - 13s - loss: 1.0134    \n",
      "Epoch 27/500\n",
      "7296/7337 [============================>.] - ETA: 0s - loss: 0.8998Epoch 00026: loss improved from 1.01344 to 0.90171, saving model to love-songs-weights-improvement-26-0.9017.hdf5\n",
      "7337/7337 [==============================] - 13s - loss: 0.9017    \n",
      "Epoch 28/500\n",
      "7296/7337 [============================>.] - ETA: 0s - loss: 0.8164Epoch 00027: loss improved from 0.90171 to 0.81525, saving model to love-songs-weights-improvement-27-0.8152.hdf5\n",
      "7337/7337 [==============================] - 13s - loss: 0.8152    \n",
      "Epoch 29/500\n",
      "7296/7337 [============================>.] - ETA: 0s - loss: 0.7375Epoch 00028: loss improved from 0.81525 to 0.73646, saving model to love-songs-weights-improvement-28-0.7365.hdf5\n",
      "7337/7337 [==============================] - 13s - loss: 0.7365    \n",
      "Epoch 30/500\n",
      "7296/7337 [============================>.] - ETA: 0s - loss: 0.6497Epoch 00029: loss improved from 0.73646 to 0.64919, saving model to love-songs-weights-improvement-29-0.6492.hdf5\n",
      "7337/7337 [==============================] - 13s - loss: 0.6492    \n",
      "Epoch 31/500\n",
      "7296/7337 [============================>.] - ETA: 0s - loss: 0.5876Epoch 00030: loss improved from 0.64919 to 0.58888, saving model to love-songs-weights-improvement-30-0.5889.hdf5\n",
      "7337/7337 [==============================] - 13s - loss: 0.5889    \n",
      "Epoch 32/500\n",
      "7296/7337 [============================>.] - ETA: 0s - loss: 0.5459Epoch 00031: loss improved from 0.58888 to 0.54535, saving model to love-songs-weights-improvement-31-0.5453.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7337/7337 [==============================] - 13s - loss: 0.5453    \n",
      "Epoch 33/500\n",
      "7296/7337 [============================>.] - ETA: 0s - loss: 0.4711Epoch 00032: loss improved from 0.54535 to 0.47194, saving model to love-songs-weights-improvement-32-0.4719.hdf5\n",
      "7337/7337 [==============================] - 13s - loss: 0.4719    \n",
      "Epoch 34/500\n",
      "7296/7337 [============================>.] - ETA: 0s - loss: 0.4338Epoch 00033: loss improved from 0.47194 to 0.43429, saving model to love-songs-weights-improvement-33-0.4343.hdf5\n",
      "7337/7337 [==============================] - 13s - loss: 0.4343    \n",
      "Epoch 35/500\n",
      "7296/7337 [============================>.] - ETA: 0s - loss: 0.3931Epoch 00034: loss improved from 0.43429 to 0.39426, saving model to love-songs-weights-improvement-34-0.3943.hdf5\n",
      "7337/7337 [==============================] - 13s - loss: 0.3943    \n",
      "Epoch 36/500\n",
      "7296/7337 [============================>.] - ETA: 0s - loss: 0.3564Epoch 00035: loss improved from 0.39426 to 0.35668, saving model to love-songs-weights-improvement-35-0.3567.hdf5\n",
      "7337/7337 [==============================] - 13s - loss: 0.3567    \n",
      "Epoch 37/500\n",
      "7296/7337 [============================>.] - ETA: 0s - loss: 0.3138Epoch 00036: loss improved from 0.35668 to 0.31346, saving model to love-songs-weights-improvement-36-0.3135.hdf5\n",
      "7337/7337 [==============================] - 13s - loss: 0.3135    \n",
      "Epoch 38/500\n",
      "7296/7337 [============================>.] - ETA: 0s - loss: 0.2906Epoch 00037: loss improved from 0.31346 to 0.29029, saving model to love-songs-weights-improvement-37-0.2903.hdf5\n",
      "7337/7337 [==============================] - 13s - loss: 0.2903    \n",
      "Epoch 39/500\n",
      "7296/7337 [============================>.] - ETA: 0s - loss: 0.2540Epoch 00038: loss improved from 0.29029 to 0.25372, saving model to love-songs-weights-improvement-38-0.2537.hdf5\n",
      "7337/7337 [==============================] - 13s - loss: 0.2537    \n",
      "Epoch 40/500\n",
      "7296/7337 [============================>.] - ETA: 0s - loss: 0.2299Epoch 00039: loss improved from 0.25372 to 0.22989, saving model to love-songs-weights-improvement-39-0.2299.hdf5\n",
      "7337/7337 [==============================] - 13s - loss: 0.2299    \n",
      "Epoch 41/500\n",
      "7296/7337 [============================>.] - ETA: 0s - loss: 0.2122Epoch 00040: loss improved from 0.22989 to 0.21190, saving model to love-songs-weights-improvement-40-0.2119.hdf5\n",
      "7337/7337 [==============================] - 14s - loss: 0.2119    \n",
      "Epoch 42/500\n",
      "3456/7337 [=============>................] - ETA: 7s - loss: 0.1834"
     ]
    }
   ],
   "source": [
    "import time \n",
    "# your code here\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    model.fit(X, y, epochs=500, batch_size=128,callbacks=callbacks_list)\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"Training took:\")\n",
    "t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rap Lyric Starting now:\n",
      "\" ting my sales on billboards yeah i shine like gold\n",
      "y'all clusters i'm taking over the universe y'all \"\n",
      " got them fake converses\n",
      "it's so worthless for y'all cause to verse jordeezy and youngdeezy\n",
      "cause they both go hard as shit jordeezy been dead and gone\n",
      "youngdeezy taken over his soul yeah i'm an asshole but girls love it\n",
      "im blasting shit\n",
      "bitches on my johnson no magic shit\n",
      "niggas talking crazy about my city i ain't having it\n",
      "talking about new york fell off, ain't that some shit\n",
      "if i ever see us falling off i bet im catching it\n",
      "paper on your head through a phone call im faxing shit\n",
      "oh and i get a quarter back and i sack a bitch\n",
      "adding bitches subtracting bitches that mathamatic shitt\n",
      "run up on your block and u git shocked when that static hit\n",
      "im acting up, u r l smack it up\n",
      "now u mad as fuck. you niggas softer than saggy nuts\n",
      "im stacking up this dough hotter than dragon butt\n",
      "my flow catching up you ugly couldn't bag a slut\n",
      "i was never used to\n",
      "being what i used to\n",
      "niggas carrying more than one strap so i used to\n",
      "always have to watch my own back so im used to\n",
      "talking to myself u would have thought i had a blue tooth\n",
      "y.u niggas hating...y.u niggas mad\n",
      "yo why you mad\n",
      "stop being a trashbag and get glad\n",
      "jordeezy is my comrade\n",
      "like we was in a war cuz everybody getting mad\n",
      "i dont claim to have 22 in my hand\n",
      "the only thing i have is sand\n",
      "cuz i'll put ya to sleep like the sandman\n",
      "30% rubber band man\n",
      "bombing pussy like a kamikaze\n",
      "every couple seconds i claim another victim\n",
      "is they survive then they suffer from stigmatism\n",
      "shut their whole body down like it was a system\n",
      "then tell them they had nothing to be mad or over\n",
      "next time you come after me make sure your sober\n",
      "but at the end of the day imma ask you\n",
      "y u mad son?\n",
      "uh, look\n",
      "i got richkid, deezy, and my dawg trae with me\n",
      "the only realest day one niggas with me\n",
      "i heard you talkin' about my music and how it ain't shit\n",
      "my nigga, when was the last time you made a hit?\n",
      "when was the last time, you got propped for a feature?\n",
      "i got bars to give you a sugar rush then drop with a seizure\n",
      "i got bars. hooks, flows, and even 808 tricks\n",
      "money, weed, lean, codeine and even yo' bitch\n",
      "so don't tryna ask me for any collaborations\n",
      "coz we did it already, it's just celebrations\n",
      "i'm got the colors of a zebra, i'm mixed\n",
      "you got ghost writers, your hooks are fixed\n",
      "we kill every track we're featured on\n",
      "i could even finish you on a one-on-one\n",
      "we're attached to this bitch like a pad\n",
      "yeah, but just tell me, y u mad?\n",
      "and look, i'm not tryna make you sound bad\n",
      "so my nigga, just tell me, y u mad?\n",
      "for this life i cannot change\n",
      "hidden hills, deep off in the main\n",
      "m&m’s  sweet like candy cane\n",
      "drop the top, pop it let it bang (pop it, pop it)\n",
      "for this life i cannot change\n",
      "hidden hills, deep off in the main\n",
      "m&m’s  sweet like candy cane\n",
      "drop the top, pop it let it bang (pop it, pop it)\n",
      "for this life i cannot change\n",
      "hidden hills, deep off in the main\n",
      "m&m’s  sweet like candy cane\n",
      "drop the top, pop it let it bang (pop it, pop it)\n",
      "for this life i cannot change\n",
      "hidden hills, deep off in the main\n",
      "m&m’s  sweet like candy cane\n",
      "drop the top, pop it let \n",
      " Hope it dropped some rhymes ;) \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# load text from the model\n",
    "\n",
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Love song Lyrics Starting now:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "\n",
    "# generate characters\n",
    "for i in range(3000):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index) \n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print (\"\\n Hope it dropped cool words ;) \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
