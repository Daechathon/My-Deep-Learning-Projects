{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rakshith\\Miniconda3\\envs\\machinelearning\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 11464384950194181277\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 6707519816\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 12719929708063655328\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# check for GPU \n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"nigga y.u mad i add the recipe to my raps you had your\\nchance i use the trance to take shit to advance i'm\\ntaking my stance i'm back on my gangsta flow\\ni don't fuck with the spam damn i be showing y'all a good t\\ntime this gonn be a summertime jam yall take exams on\\nmy swag cause i'm gonna drag y'all to the bottom\\ny'all smell like rotten milk i killed y'all motto i killed the lotto\\nthrow sum mo hundreds throw that pussy back bitch i\\nswitch up my flow cause i'm one of a kind i be on my\\nmastermind grind i got rid of the whack niggas that's y.u\\nmad that's y.u sad watch me break it down i run this town\\ni breakdown the barz like a tic tac toe you a hoe my bro\\njrizzy and brainbusta stay killin every rap nigga that ome\\nface us girl put that face down and your ass up i be\\nin da club getting my groove on i move like jagger\\ni'm the one and only rapper that i be on my cool shit\\nthis song y.u mad it's so legit that i let desperate bitches\\nmassages my dick my topic is so sick y'all some tragic niggas\\nthat's y.u mad i be getting my sales on billboards yeah i shine like gold\\ny'all clusters i'm taking over the universe y'all got them fake converses\\nit's so worthless for y'all cause to verse jordeezy and youngdeezy\\ncause they both go hard as shit jordeezy been dead and gone\\nyoungdeezy taken over his soul yeah i'm an asshole but girls love it\\nim blasting shit\\nbitches on my johnson no magic shit\\nniggas talking crazy about my city i ain't having it\\ntalking about new york fell off, ain't that some shit\\nif i ever see us falling off i bet im catching it\\npaper on your head through a phone call im faxing shit\\noh and i get a quarter back and i sack a bitch\\nadding bitches subtracting bitches that mathamatic shitt\\nrun up on your block and u get shocked when that static hit\\nim acting up, u r l smack it up\\nnow u mad as fuck. you niggas softer than saggy nuts\\nim stacking up this dough hotter than dragon butt\\nmy flow catching up you ugly couldn't bag a slut\\ni was never used to\\nbeing what i used to\\nniggas carrying more than one strap so i used to\\nalways have to watch my own back so im used to\\ntalking to myself u would have thought i had a blue tooth\\ny.u niggas hating...y.u niggas mad\\nyo why you mad\\nstop being a trashbag and get glad\\njordeezy is my comrade\\nlike we was in a war cuz everybody getting mad\\ni dont claim to have 22 in my hand\\nthe only thing i have is sand\\ncuz i'll put ya to sleep like the sandman\\n30% rubber band man\\nbombing pussy like a kamikaze\\nevery couple seconds i claim another victim\\nis they survive then they suffer from stigmatism\\nshut their whole body down like it was a system\\nthen tell them they had nothing to be mad or over\\nnext time you come after me make sure your sober\\nbut at the end of the day imma ask you\\ny u mad son?\\nuh, look\\ni got richkid, deezy, and my dawg trae with me\\nthe only realest day one niggas with me\\ni heard you talkin' about my music and how it ain't shit\\nmy nigga, when was the last time you made a hit?\\nwhen was the last time, you got propped for a feature?\\ni got bars to give you a sugar rush then drop with a seizure\\ni got bars. hooks, flows, and even 808 tricks\\nmoney, weed, lean, codeine and even yo' bitch\\nso don't tryna ask me for any collaborations\\ncoz we did it already, it's just celebrations\\ni'm got the colors of a zebra, i'm mixed\\nyou got ghost writers, your hooks are fixed\\nwe kill every track we're featured on\\ni could even finish you on a one-on-one\\nwe're attached to this bitch like a pad\\nyeah, but just tell me, y u mad?\\nand look, i'm not tryna make you sound bad\\nso my nigga, just tell me, y u mad?\\nfor this life i cannot change\\nhidden hills, deep off in the main\\nm&m’s, sweet like candy cane\\ndrop the top, pop it let it bang (pop it, pop it)\\nfor this life i cannot change\\nhidden hills, deep off in the main\\nm&m’s sweet like candy cane\\ndrop the top, pop it let it bang (pop it, pop it)\\ndrop the top play hide and seek\\njump inside, jump straight to the leak\\ntake a sip, feel just how i breathe\\non freeway, but no ain’t nothin’ free (straight up)\\nbend laws, bend lanes\\ndaylight, i wake up feeling like you won't play right\\ni used to know but now, that shit don't feel right\\nit made me put away my pride\\nso long, you made a nigga wait for some, so long\\nyou make it hard for a boy like that to go on\\ni'm wishing i could make this mine, oh\\nif you want it, yeah\\nyou can have it, oh, oh, oh\\nif you need it, ooh\\nwe can make it, oh\\nif you want it\\nyou can have it\\nbut stay woke\\nniggas creepin'\\nthey gon' find you\\ngon' catch you sleepin'\\nooh, now stay woke\\nniggas creepin'\\nnow don't you close your eyes\\ntoo late\\nyou wanna make it right, but now it's too late\\nmy peanut butter chocolate cake with kool-aid\\ni'm trying not to waste my time\\nif you want it, oh\\nyou can have it, you can have it\\nif you need it\\nyou better believe in something\\nwe can make it\\nif you want it\\nyou can have it, aah!\\nbut stay woke\\nniggas creepin'\\nthey gon' find you\\ngon' catch you sleepin'\\nput your hands up on me\\nnow stay woke\\nniggas creepin'\\nnow don't you close your eyes\\nbut stay woke, ooh\\nniggas creepin'\\nthey gon' find you\\ngon' catch you sleepin'\\nooh, now stay woke\\nniggas creepin'\\nnow don't you close your eyes\\nbaby get so scandalous, oh\\nhow'd it get so scandalous?\\noh, oh\\nbaby you\\nhow'd it get...\\nhow'd it get so scandalous?\\nooh we get so scandalous\\nbut stay woke\\nbut stay woke\\nbeen bustin’ bills, but still ain’t nothin’ change\\nyou in the mob soon as you rock the chain\\nshe caught the waves just thumbin’ through my braids (alright)\\nheatin’ up, baby, i’m just heatin’ up (it’s lit)\\nfiji lover, now they indeed that is impressive\\nfill his cup now you know how they keep me up\\nicy love, icy like a hockey puck\\nfor this life i cannot change\\nhidden hills, deep off in the main\\nm&m’s sweet like candy cane\\ndrop the top, pop it let it bang (pop it, pop it)\\nfor this life i cannot change\\nhidden hills, deep off in the main\\nm&m’s sweet like candy cane\\ndrop the top, pop it let it bang (pop it, pop it)\\nall the ones, all the chains piled on the mantle\\nall the dogs, all the dogs low creep right behind me in the phantom\\nnever go, never go dip on this estates santana\\nyeah, run it back turn the lights on when i hit up green lantern\\nyeah, fly the broads, fly the dogs down to atlanta\\nyeah, in the cut in medusa lay low i might be\\nyeah, roll up help me calm down when i move in high speed\\nyeah, if i send one need the text back ’cause you know what i need\\noh please (oh please)\\noh me (oh me)\\noh my (oh my)\\nwe been movin’, we been movin’ for some time (alright!)\\nflexin’, flexin’ try to exercise\\nexercise, exercise, exercise, exercise\\nfor this life i cannot change\\nhidden hills, deep off in the main\\nm&m’s sweet like candy cane\\ndrop the top, pop it let it bang (pop it, pop it)\\nfor this life i cannot change\\nhidden hills, deep off in the main\\nm&m’s sweet like candy cane\\ndrop the top, pop it let it bang (yeah)\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "filename = \"butterefly.txt\"\n",
    "raw_text = open(filename,encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()\n",
    "raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get all the chars in the sequence of text\n",
    "\n",
    "chars = sorted(list(set(raw_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert to dicts \n",
    "\n",
    "char_to_int = {ch:i for i,ch in enumerate(chars)}\n",
    "int_to_char = {i:ch for i,ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters: 6841 \n",
      "Total Vocab: 43\n"
     ]
    }
   ],
   "source": [
    "# setup the variables used later \n",
    "\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print (\"Total Characters: {} \".format(n_chars))\n",
    "print (\"Total Vocab: {}\".format(n_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  6741\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18,\n",
       " 20,\n",
       " 1,\n",
       " 24,\n",
       " 5,\n",
       " 28,\n",
       " 0,\n",
       " 35,\n",
       " 16,\n",
       " 26,\n",
       " 24,\n",
       " 29,\n",
       " 22,\n",
       " 1,\n",
       " 28,\n",
       " 40,\n",
       " 1,\n",
       " 34,\n",
       " 35,\n",
       " 16,\n",
       " 29,\n",
       " 18,\n",
       " 20,\n",
       " 1,\n",
       " 24,\n",
       " 5,\n",
       " 28,\n",
       " 1,\n",
       " 17,\n",
       " 16,\n",
       " 18,\n",
       " 26,\n",
       " 1,\n",
       " 30,\n",
       " 29,\n",
       " 1,\n",
       " 28,\n",
       " 40,\n",
       " 1,\n",
       " 22,\n",
       " 16,\n",
       " 29,\n",
       " 22,\n",
       " 34,\n",
       " 35,\n",
       " 16,\n",
       " 1,\n",
       " 21,\n",
       " 27,\n",
       " 30,\n",
       " 38,\n",
       " 0,\n",
       " 24,\n",
       " 1,\n",
       " 19,\n",
       " 30,\n",
       " 29,\n",
       " 5,\n",
       " 35,\n",
       " 1,\n",
       " 21,\n",
       " 36,\n",
       " 18,\n",
       " 26,\n",
       " 1,\n",
       " 38,\n",
       " 24,\n",
       " 35,\n",
       " 23,\n",
       " 1,\n",
       " 35,\n",
       " 23,\n",
       " 20,\n",
       " 1,\n",
       " 34,\n",
       " 31,\n",
       " 16,\n",
       " 28,\n",
       " 1,\n",
       " 19,\n",
       " 16,\n",
       " 28,\n",
       " 29,\n",
       " 1,\n",
       " 24,\n",
       " 1,\n",
       " 17,\n",
       " 20,\n",
       " 1,\n",
       " 34,\n",
       " 23,\n",
       " 30,\n",
       " 38,\n",
       " 24,\n",
       " 29,\n",
       " 22,\n",
       " 1,\n",
       " 40,\n",
       " 5,\n",
       " 16,\n",
       " 27,\n",
       " 27,\n",
       " 1,\n",
       " 16,\n",
       " 1,\n",
       " 22,\n",
       " 30,\n",
       " 30,\n",
       " 19,\n",
       " 1,\n",
       " 35,\n",
       " 0,\n",
       " 35,\n",
       " 24,\n",
       " 28,\n",
       " 20,\n",
       " 1,\n",
       " 35,\n",
       " 23,\n",
       " 24,\n",
       " 34,\n",
       " 1,\n",
       " 22,\n",
       " 30,\n",
       " 29,\n",
       " 29,\n",
       " 1,\n",
       " 17,\n",
       " 20,\n",
       " 1,\n",
       " 16,\n",
       " 1,\n",
       " 34,\n",
       " 36,\n",
       " 28,\n",
       " 28,\n",
       " 20,\n",
       " 33,\n",
       " 35,\n",
       " 24,\n",
       " 28,\n",
       " 20,\n",
       " 1,\n",
       " 25,\n",
       " 16,\n",
       " 28,\n",
       " 1,\n",
       " 40,\n",
       " 16,\n",
       " 27,\n",
       " 27,\n",
       " 1,\n",
       " 35,\n",
       " 16,\n",
       " 26,\n",
       " 20,\n",
       " 1,\n",
       " 20,\n",
       " 39,\n",
       " 16,\n",
       " 28,\n",
       " 34,\n",
       " 1,\n",
       " 30,\n",
       " 29,\n",
       " 0,\n",
       " 28,\n",
       " 40,\n",
       " 1,\n",
       " 34,\n",
       " 38,\n",
       " 16,\n",
       " 22,\n",
       " 1,\n",
       " 18,\n",
       " 16,\n",
       " 36,\n",
       " 34,\n",
       " 20,\n",
       " 1,\n",
       " 24,\n",
       " 5,\n",
       " 28,\n",
       " 1,\n",
       " 22,\n",
       " 30,\n",
       " 29,\n",
       " 29,\n",
       " 16,\n",
       " 1,\n",
       " 19,\n",
       " 33,\n",
       " 16,\n",
       " 22,\n",
       " 1,\n",
       " 40,\n",
       " 5,\n",
       " 16,\n",
       " 27,\n",
       " 27,\n",
       " 1,\n",
       " 35,\n",
       " 30,\n",
       " 1,\n",
       " 35,\n",
       " 23,\n",
       " 20,\n",
       " 1,\n",
       " 17,\n",
       " 30,\n",
       " 35,\n",
       " 35,\n",
       " 30,\n",
       " 28,\n",
       " 0,\n",
       " 40,\n",
       " 5,\n",
       " 16,\n",
       " 27,\n",
       " 27,\n",
       " 1,\n",
       " 34,\n",
       " 28,\n",
       " 20,\n",
       " 27,\n",
       " 27,\n",
       " 1,\n",
       " 27,\n",
       " 24,\n",
       " 26,\n",
       " 20,\n",
       " 1,\n",
       " 33,\n",
       " 30,\n",
       " 35,\n",
       " 35,\n",
       " 20,\n",
       " 29,\n",
       " 1,\n",
       " 28,\n",
       " 24,\n",
       " 27,\n",
       " 26,\n",
       " 1,\n",
       " 24,\n",
       " 1,\n",
       " 26,\n",
       " 24,\n",
       " 27,\n",
       " 27,\n",
       " 20,\n",
       " 19,\n",
       " 1,\n",
       " 40,\n",
       " 5,\n",
       " 16,\n",
       " 27,\n",
       " 27,\n",
       " 1,\n",
       " 28,\n",
       " 30,\n",
       " 35,\n",
       " 35,\n",
       " 30,\n",
       " 1,\n",
       " 24,\n",
       " 1,\n",
       " 26,\n",
       " 24,\n",
       " 27,\n",
       " 27,\n",
       " 20,\n",
       " 19,\n",
       " 1,\n",
       " 35,\n",
       " 23,\n",
       " 20,\n",
       " 1,\n",
       " 27,\n",
       " 30,\n",
       " 35,\n",
       " 35,\n",
       " 30,\n",
       " 0,\n",
       " 35,\n",
       " 23,\n",
       " 33,\n",
       " 30,\n",
       " 38,\n",
       " 1,\n",
       " 34,\n",
       " 36,\n",
       " 28,\n",
       " 1,\n",
       " 28,\n",
       " 30,\n",
       " 1,\n",
       " 23,\n",
       " 36,\n",
       " 29,\n",
       " 19,\n",
       " 33,\n",
       " 20,\n",
       " 19,\n",
       " 34,\n",
       " 1,\n",
       " 35,\n",
       " 23,\n",
       " 33,\n",
       " 30,\n",
       " 38,\n",
       " 1,\n",
       " 35,\n",
       " 23,\n",
       " 16,\n",
       " 35,\n",
       " 1,\n",
       " 31,\n",
       " 36,\n",
       " 34,\n",
       " 34,\n",
       " 40,\n",
       " 1,\n",
       " 17,\n",
       " 16,\n",
       " 18,\n",
       " 26,\n",
       " 1,\n",
       " 17,\n",
       " 24,\n",
       " 35,\n",
       " 18,\n",
       " 23,\n",
       " 1,\n",
       " 24,\n",
       " 0,\n",
       " 34,\n",
       " 38,\n",
       " 24,\n",
       " 35,\n",
       " 18,\n",
       " 23,\n",
       " 1,\n",
       " 36,\n",
       " 31,\n",
       " 1,\n",
       " 28,\n",
       " 40,\n",
       " 1,\n",
       " 21,\n",
       " 27,\n",
       " 30,\n",
       " 38,\n",
       " 1,\n",
       " 18,\n",
       " 16,\n",
       " 36,\n",
       " 34,\n",
       " 20,\n",
       " 1,\n",
       " 24,\n",
       " 5,\n",
       " 28,\n",
       " 1,\n",
       " 30,\n",
       " 29,\n",
       " 20,\n",
       " 1,\n",
       " 30,\n",
       " 21,\n",
       " 1,\n",
       " 16,\n",
       " 1,\n",
       " 26,\n",
       " 24,\n",
       " 29,\n",
       " 19,\n",
       " 1,\n",
       " 24,\n",
       " 1,\n",
       " 17,\n",
       " 20,\n",
       " 1,\n",
       " 30,\n",
       " 29,\n",
       " 1,\n",
       " 28,\n",
       " 40,\n",
       " 0,\n",
       " 28,\n",
       " 16,\n",
       " 34,\n",
       " 35,\n",
       " 20,\n",
       " 33,\n",
       " 28,\n",
       " 24,\n",
       " 29,\n",
       " 19,\n",
       " 1,\n",
       " 22,\n",
       " 33,\n",
       " 24,\n",
       " 29,\n",
       " 19,\n",
       " 1,\n",
       " 24,\n",
       " 1,\n",
       " 22,\n",
       " 30,\n",
       " 35,\n",
       " 1,\n",
       " 33,\n",
       " 24,\n",
       " 19,\n",
       " 1,\n",
       " 30,\n",
       " 21,\n",
       " 1,\n",
       " 35,\n",
       " 23,\n",
       " 20,\n",
       " 1,\n",
       " 38,\n",
       " 23,\n",
       " 16,\n",
       " 18,\n",
       " 26,\n",
       " 1,\n",
       " 29,\n",
       " 24,\n",
       " 22,\n",
       " 22,\n",
       " 16,\n",
       " 34,\n",
       " 1,\n",
       " 35,\n",
       " 23,\n",
       " 16,\n",
       " 35,\n",
       " 5,\n",
       " 34,\n",
       " 1,\n",
       " 40,\n",
       " 10,\n",
       " 36,\n",
       " 0,\n",
       " 28,\n",
       " 16,\n",
       " 19,\n",
       " 1,\n",
       " 35,\n",
       " 23,\n",
       " 16,\n",
       " 35,\n",
       " 5,\n",
       " 34,\n",
       " 1,\n",
       " 40,\n",
       " 10,\n",
       " 36,\n",
       " 1,\n",
       " 34,\n",
       " 16,\n",
       " 19,\n",
       " 1,\n",
       " 38,\n",
       " 16,\n",
       " 35,\n",
       " 18,\n",
       " 23,\n",
       " 1,\n",
       " 28,\n",
       " 20,\n",
       " 1,\n",
       " 17,\n",
       " 33,\n",
       " 20,\n",
       " 16,\n",
       " 26,\n",
       " 1,\n",
       " 24,\n",
       " 35,\n",
       " 1,\n",
       " 19,\n",
       " 30,\n",
       " 38,\n",
       " 29,\n",
       " 1,\n",
       " 24,\n",
       " 1,\n",
       " 33,\n",
       " 36,\n",
       " 29,\n",
       " 1,\n",
       " 35,\n",
       " 23,\n",
       " 24,\n",
       " 34,\n",
       " 1,\n",
       " 35,\n",
       " 30,\n",
       " 38,\n",
       " 29,\n",
       " 0,\n",
       " 24,\n",
       " 1,\n",
       " 17,\n",
       " 33,\n",
       " 20,\n",
       " 16,\n",
       " 26,\n",
       " 19,\n",
       " 30,\n",
       " 38,\n",
       " 29,\n",
       " 1,\n",
       " 35,\n",
       " 23,\n",
       " 20,\n",
       " 1,\n",
       " 17,\n",
       " 16,\n",
       " 33,\n",
       " 41,\n",
       " 1,\n",
       " 27,\n",
       " 24,\n",
       " 26,\n",
       " 20,\n",
       " 1,\n",
       " 16,\n",
       " 1,\n",
       " 35,\n",
       " 24,\n",
       " 18,\n",
       " 1,\n",
       " 35,\n",
       " 16,\n",
       " 18,\n",
       " 1,\n",
       " 35,\n",
       " 30,\n",
       " 20,\n",
       " 1,\n",
       " 40,\n",
       " 30,\n",
       " 36,\n",
       " 1,\n",
       " 16,\n",
       " 1,\n",
       " 23,\n",
       " 30,\n",
       " 20,\n",
       " 1,\n",
       " 28,\n",
       " 40,\n",
       " 1,\n",
       " 17,\n",
       " 33,\n",
       " 30,\n",
       " 0,\n",
       " 25,\n",
       " 33,\n",
       " 24,\n",
       " 41,\n",
       " 41,\n",
       " 40,\n",
       " 1,\n",
       " 16,\n",
       " 29,\n",
       " 19,\n",
       " 1,\n",
       " 17,\n",
       " 33,\n",
       " 16,\n",
       " 24,\n",
       " 29,\n",
       " 17,\n",
       " 36,\n",
       " 34,\n",
       " 35,\n",
       " 16,\n",
       " 1,\n",
       " 34,\n",
       " 35,\n",
       " 16,\n",
       " 40,\n",
       " 1,\n",
       " 26,\n",
       " 24,\n",
       " 27,\n",
       " 27,\n",
       " 24,\n",
       " 29,\n",
       " 1,\n",
       " 20,\n",
       " 37,\n",
       " 20,\n",
       " 33,\n",
       " 40,\n",
       " 1,\n",
       " 33,\n",
       " 16,\n",
       " 31,\n",
       " 1,\n",
       " 29,\n",
       " 24,\n",
       " 22,\n",
       " 22,\n",
       " 16,\n",
       " 1,\n",
       " 35,\n",
       " 23,\n",
       " 16,\n",
       " 35,\n",
       " 1,\n",
       " 30,\n",
       " 28,\n",
       " 20,\n",
       " 0,\n",
       " 21,\n",
       " 16,\n",
       " 18,\n",
       " 20,\n",
       " 1,\n",
       " 36,\n",
       " 34,\n",
       " 1,\n",
       " 22,\n",
       " 24,\n",
       " 33,\n",
       " 27,\n",
       " 1,\n",
       " 31,\n",
       " 36,\n",
       " 35,\n",
       " 1,\n",
       " 35,\n",
       " 23,\n",
       " 16,\n",
       " 35,\n",
       " 1,\n",
       " 21,\n",
       " 16,\n",
       " 18,\n",
       " 20,\n",
       " 1,\n",
       " 19,\n",
       " 30,\n",
       " 38,\n",
       " 29,\n",
       " 1,\n",
       " 16,\n",
       " 29,\n",
       " 19,\n",
       " 1,\n",
       " 40,\n",
       " 30,\n",
       " 36,\n",
       " 33,\n",
       " 1,\n",
       " 16,\n",
       " 34,\n",
       " 34,\n",
       " 1,\n",
       " 36,\n",
       " 31,\n",
       " 1,\n",
       " 24,\n",
       " 1,\n",
       " 17,\n",
       " 20,\n",
       " 0,\n",
       " 24,\n",
       " 29,\n",
       " 1,\n",
       " 19,\n",
       " 16,\n",
       " 1,\n",
       " 18,\n",
       " 27,\n",
       " 36,\n",
       " 17,\n",
       " 1,\n",
       " 22,\n",
       " 20,\n",
       " 35,\n",
       " 35,\n",
       " 24,\n",
       " 29,\n",
       " 22,\n",
       " 1,\n",
       " 28,\n",
       " 40,\n",
       " 1,\n",
       " 22,\n",
       " 33,\n",
       " 30,\n",
       " 30,\n",
       " 37,\n",
       " 20,\n",
       " 1,\n",
       " 30,\n",
       " 29,\n",
       " 1,\n",
       " 24,\n",
       " 1,\n",
       " 28,\n",
       " 30,\n",
       " 37,\n",
       " 20,\n",
       " 1,\n",
       " 27,\n",
       " 24,\n",
       " 26,\n",
       " 20,\n",
       " 1,\n",
       " 25,\n",
       " 16,\n",
       " 22,\n",
       " 22,\n",
       " 20,\n",
       " 33,\n",
       " 0,\n",
       " 24,\n",
       " 5,\n",
       " 28,\n",
       " 1,\n",
       " 35,\n",
       " 23,\n",
       " 20,\n",
       " 1,\n",
       " 30,\n",
       " 29,\n",
       " 20,\n",
       " 1,\n",
       " 16,\n",
       " 29,\n",
       " 19,\n",
       " 1,\n",
       " 30,\n",
       " 29,\n",
       " 27,\n",
       " 40,\n",
       " 1,\n",
       " 33,\n",
       " 16,\n",
       " 31,\n",
       " 31,\n",
       " 20,\n",
       " 33,\n",
       " 1,\n",
       " 35,\n",
       " 23,\n",
       " 16,\n",
       " 35,\n",
       " 1,\n",
       " 24,\n",
       " 1,\n",
       " 17,\n",
       " 20,\n",
       " 1,\n",
       " 30,\n",
       " 29,\n",
       " 1,\n",
       " 28,\n",
       " 40,\n",
       " 1,\n",
       " 18,\n",
       " 30,\n",
       " 30,\n",
       " 27,\n",
       " 1,\n",
       " 34,\n",
       " 23,\n",
       " 24,\n",
       " 35,\n",
       " 0,\n",
       " 35,\n",
       " 23,\n",
       " 24,\n",
       " 34,\n",
       " 1,\n",
       " 34,\n",
       " 30,\n",
       " 29,\n",
       " 22,\n",
       " 1,\n",
       " 40,\n",
       " 10,\n",
       " 36,\n",
       " 1,\n",
       " 28,\n",
       " 16,\n",
       " 19,\n",
       " 1,\n",
       " 24,\n",
       " 35,\n",
       " 5,\n",
       " 34,\n",
       " 1,\n",
       " 34,\n",
       " 30,\n",
       " 1,\n",
       " 27,\n",
       " 20,\n",
       " 22,\n",
       " 24,\n",
       " 35,\n",
       " 1,\n",
       " 35,\n",
       " 23,\n",
       " 16,\n",
       " 35,\n",
       " 1,\n",
       " 24,\n",
       " 1,\n",
       " 27,\n",
       " 20,\n",
       " 35,\n",
       " 1,\n",
       " 19,\n",
       " 20,\n",
       " 34,\n",
       " 31,\n",
       " 20,\n",
       " 33,\n",
       " 16,\n",
       " 35,\n",
       " 20,\n",
       " 1,\n",
       " 17,\n",
       " 24,\n",
       " 35,\n",
       " 18,\n",
       " 23,\n",
       " 20,\n",
       " 34,\n",
       " 0,\n",
       " 28,\n",
       " 16,\n",
       " 34,\n",
       " 34,\n",
       " 16,\n",
       " 22,\n",
       " 20,\n",
       " 34,\n",
       " 1,\n",
       " 28,\n",
       " 40,\n",
       " 1,\n",
       " 19,\n",
       " 24,\n",
       " 18,\n",
       " 26,\n",
       " 1,\n",
       " 28,\n",
       " 40,\n",
       " 1,\n",
       " 35,\n",
       " 30,\n",
       " 31,\n",
       " 24,\n",
       " 18,\n",
       " 1,\n",
       " 24,\n",
       " 34,\n",
       " 1,\n",
       " 34,\n",
       " 30,\n",
       " 1,\n",
       " 34,\n",
       " 24,\n",
       " 18,\n",
       " 26,\n",
       " 1,\n",
       " 40,\n",
       " 5,\n",
       " 16,\n",
       " 27,\n",
       " 27,\n",
       " 1,\n",
       " 34,\n",
       " 30,\n",
       " 28,\n",
       " 20,\n",
       " 1,\n",
       " 35,\n",
       " 33,\n",
       " 16,\n",
       " 22,\n",
       " 24,\n",
       " 18,\n",
       " 1,\n",
       " 29,\n",
       " 24,\n",
       " 22,\n",
       " 22,\n",
       " 16,\n",
       " 34,\n",
       " 0,\n",
       " 35,\n",
       " 23,\n",
       " 16,\n",
       " 35,\n",
       " 5,\n",
       " 34,\n",
       " 1,\n",
       " 40,\n",
       " 10,\n",
       " 36,\n",
       " 1,\n",
       " 28,\n",
       " 16,\n",
       " 19,\n",
       " 1,\n",
       " 24,\n",
       " 1,\n",
       " 17,\n",
       " 20,\n",
       " 1,\n",
       " 22,\n",
       " 20,\n",
       " 35,\n",
       " 35,\n",
       " 24,\n",
       " 29,\n",
       " 22,\n",
       " 1,\n",
       " 28,\n",
       " 40,\n",
       " 1,\n",
       " 34,\n",
       " 16,\n",
       " 27,\n",
       " 20,\n",
       " 34,\n",
       " 1,\n",
       " 30,\n",
       " 29,\n",
       " 1,\n",
       " 17,\n",
       " 24,\n",
       " 27,\n",
       " 27,\n",
       " 17,\n",
       " 30,\n",
       " 16,\n",
       " 33,\n",
       " 19,\n",
       " 34,\n",
       " 1,\n",
       " 40,\n",
       " 20,\n",
       " 16,\n",
       " 23,\n",
       " 1,\n",
       " 24,\n",
       " 1,\n",
       " 34,\n",
       " 23,\n",
       " 24,\n",
       " 29,\n",
       " 20,\n",
       " 1,\n",
       " 27,\n",
       " 24,\n",
       " 26,\n",
       " 20,\n",
       " 1,\n",
       " 22,\n",
       " 30,\n",
       " 27,\n",
       " 19,\n",
       " 0,\n",
       " 40,\n",
       " 5,\n",
       " 16,\n",
       " 27,\n",
       " 27,\n",
       " 1,\n",
       " 18,\n",
       " 27,\n",
       " 36,\n",
       " 34,\n",
       " 35,\n",
       " 20,\n",
       " 33,\n",
       " 34,\n",
       " 1,\n",
       " 24,\n",
       " 5,\n",
       " 28,\n",
       " 1,\n",
       " 35,\n",
       " 16,\n",
       " 26,\n",
       " 24,\n",
       " 29,\n",
       " ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Rakshith\\Miniconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1192: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From C:\\Users\\Rakshith\\Miniconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1299: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "# define the LSTM model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "\n",
    "filepath=\"t4-eminem-weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from previous epoch - 1ooth epoch\n",
    "# load the network weights\n",
    "# filename = \"eminem-weights-improvement-13-2.2727.hdf5\"\n",
    "# model.load_weights(filename)\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 3.1871Epoch 00000: loss improved from inf to 3.18716, saving model to t4-eminem-weights-improvement-00-3.1872.hdf5\n",
      "6741/6741 [==============================] - 21s - loss: 3.1872    \n",
      "Epoch 2/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 3.0701Epoch 00001: loss improved from 3.18716 to 3.07101, saving model to t4-eminem-weights-improvement-01-3.0710.hdf5\n",
      "6741/6741 [==============================] - 18s - loss: 3.0710    \n",
      "Epoch 3/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 3.0669Epoch 00002: loss improved from 3.07101 to 3.06788, saving model to t4-eminem-weights-improvement-02-3.0679.hdf5\n",
      "6741/6741 [==============================] - 19s - loss: 3.0679    \n",
      "Epoch 4/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 3.0632Epoch 00003: loss improved from 3.06788 to 3.06505, saving model to t4-eminem-weights-improvement-03-3.0651.hdf5\n",
      "6741/6741 [==============================] - 19s - loss: 3.0651    \n",
      "Epoch 5/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 3.0604Epoch 00004: loss improved from 3.06505 to 3.06050, saving model to t4-eminem-weights-improvement-04-3.0605.hdf5\n",
      "6741/6741 [==============================] - 18s - loss: 3.0605    \n",
      "Epoch 6/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 3.0268Epoch 00005: loss improved from 3.06050 to 3.02791, saving model to t4-eminem-weights-improvement-05-3.0279.hdf5\n",
      "6741/6741 [==============================] - 19s - loss: 3.0279    \n",
      "Epoch 7/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 2.9842Epoch 00006: loss improved from 3.02791 to 2.98461, saving model to t4-eminem-weights-improvement-06-2.9846.hdf5\n",
      "6741/6741 [==============================] - 18s - loss: 2.9846    \n",
      "Epoch 8/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 2.9441Epoch 00007: loss improved from 2.98461 to 2.94299, saving model to t4-eminem-weights-improvement-07-2.9430.hdf5\n",
      "6741/6741 [==============================] - 19s - loss: 2.9430    \n",
      "Epoch 9/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 2.8874Epoch 00008: loss improved from 2.94299 to 2.88765, saving model to t4-eminem-weights-improvement-08-2.8877.hdf5\n",
      "6741/6741 [==============================] - 20s - loss: 2.8877    \n",
      "Epoch 10/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 2.8614Epoch 00009: loss improved from 2.88765 to 2.86245, saving model to t4-eminem-weights-improvement-09-2.8624.hdf5\n",
      "6741/6741 [==============================] - 18s - loss: 2.8624    \n",
      "Epoch 11/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 2.8344Epoch 00010: loss improved from 2.86245 to 2.83546, saving model to t4-eminem-weights-improvement-10-2.8355.hdf5\n",
      "6741/6741 [==============================] - 19s - loss: 2.8355    \n",
      "Epoch 12/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 2.8184Epoch 00011: loss improved from 2.83546 to 2.81826, saving model to t4-eminem-weights-improvement-11-2.8183.hdf5\n",
      "6741/6741 [==============================] - 18s - loss: 2.8183    \n",
      "Epoch 13/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 2.8058Epoch 00012: loss improved from 2.81826 to 2.80520, saving model to t4-eminem-weights-improvement-12-2.8052.hdf5\n",
      "6741/6741 [==============================] - 18s - loss: 2.8052    \n",
      "Epoch 14/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 2.7837Epoch 00013: loss improved from 2.80520 to 2.78594, saving model to t4-eminem-weights-improvement-13-2.7859.hdf5\n",
      "6741/6741 [==============================] - 18s - loss: 2.7859    \n",
      "Epoch 15/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 2.7672Epoch 00014: loss improved from 2.78594 to 2.76894, saving model to t4-eminem-weights-improvement-14-2.7689.hdf5\n",
      "6741/6741 [==============================] - 18s - loss: 2.7689    \n",
      "Epoch 16/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 2.7539Epoch 00015: loss improved from 2.76894 to 2.75204, saving model to t4-eminem-weights-improvement-15-2.7520.hdf5\n",
      "6741/6741 [==============================] - 18s - loss: 2.7520    \n",
      "Epoch 17/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 2.7288Epoch 00016: loss improved from 2.75204 to 2.72884, saving model to t4-eminem-weights-improvement-16-2.7288.hdf5\n",
      "6741/6741 [==============================] - 18s - loss: 2.7288    \n",
      "Epoch 18/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 2.7033Epoch 00017: loss improved from 2.72884 to 2.70491, saving model to t4-eminem-weights-improvement-17-2.7049.hdf5\n",
      "6741/6741 [==============================] - 19s - loss: 2.7049    \n",
      "Epoch 19/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 2.6664Epoch 00018: loss improved from 2.70491 to 2.67019, saving model to t4-eminem-weights-improvement-18-2.6702.hdf5\n",
      "6741/6741 [==============================] - 18s - loss: 2.6702    \n",
      "Epoch 20/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 2.6403Epoch 00019: loss improved from 2.67019 to 2.64197, saving model to t4-eminem-weights-improvement-19-2.6420.hdf5\n",
      "6741/6741 [==============================] - 18s - loss: 2.6420    \n",
      "Epoch 21/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 2.5992Epoch 00020: loss improved from 2.64197 to 2.59804, saving model to t4-eminem-weights-improvement-20-2.5980.hdf5\n",
      "6741/6741 [==============================] - 18s - loss: 2.5980    \n",
      "Epoch 22/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 2.5531Epoch 00021: loss improved from 2.59804 to 2.55268, saving model to t4-eminem-weights-improvement-21-2.5527.hdf5\n",
      "6741/6741 [==============================] - 18s - loss: 2.5527    \n",
      "Epoch 23/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 2.4974Epoch 00022: loss improved from 2.55268 to 2.49882, saving model to t4-eminem-weights-improvement-22-2.4988.hdf5\n",
      "6741/6741 [==============================] - 19s - loss: 2.4988    \n",
      "Epoch 24/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 2.4478Epoch 00023: loss improved from 2.49882 to 2.44978, saving model to t4-eminem-weights-improvement-23-2.4498.hdf5\n",
      "6741/6741 [==============================] - 18s - loss: 2.4498    \n",
      "Epoch 25/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 2.3659Epoch 00024: loss improved from 2.44978 to 2.36511, saving model to t4-eminem-weights-improvement-24-2.3651.hdf5\n",
      "6741/6741 [==============================] - 21s - loss: 2.3651    \n",
      "Epoch 26/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 2.2895Epoch 00025: loss improved from 2.36511 to 2.29145, saving model to t4-eminem-weights-improvement-25-2.2914.hdf5\n",
      "6741/6741 [==============================] - 18s - loss: 2.2914    \n",
      "Epoch 27/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 2.2098Epoch 00026: loss improved from 2.29145 to 2.20800, saving model to t4-eminem-weights-improvement-26-2.2080.hdf5\n",
      "6741/6741 [==============================] - 18s - loss: 2.2080    \n",
      "Epoch 28/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 2.1188Epoch 00027: loss improved from 2.20800 to 2.11788, saving model to t4-eminem-weights-improvement-27-2.1179.hdf5\n",
      "6741/6741 [==============================] - 18s - loss: 2.1179    \n",
      "Epoch 29/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 2.0442Epoch 00028: loss improved from 2.11788 to 2.04083, saving model to t4-eminem-weights-improvement-28-2.0408.hdf5\n",
      "6741/6741 [==============================] - 18s - loss: 2.0408    \n",
      "Epoch 30/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 1.9504Epoch 00029: loss improved from 2.04083 to 1.95122, saving model to t4-eminem-weights-improvement-29-1.9512.hdf5\n",
      "6741/6741 [==============================] - 19s - loss: 1.9512    \n",
      "Epoch 31/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 1.8662Epoch 00030: loss improved from 1.95122 to 1.86444, saving model to t4-eminem-weights-improvement-30-1.8644.hdf5\n",
      "6741/6741 [==============================] - 18s - loss: 1.8644    \n",
      "Epoch 32/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 1.7742Epoch 00031: loss improved from 1.86444 to 1.77539, saving model to t4-eminem-weights-improvement-31-1.7754.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6741/6741 [==============================] - 18s - loss: 1.7754    \n",
      "Epoch 33/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 1.6798Epoch 00032: loss improved from 1.77539 to 1.67893, saving model to t4-eminem-weights-improvement-32-1.6789.hdf5\n",
      "6741/6741 [==============================] - 19s - loss: 1.6789    \n",
      "Epoch 34/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 1.6030Epoch 00033: loss improved from 1.67893 to 1.60162, saving model to t4-eminem-weights-improvement-33-1.6016.hdf5\n",
      "6741/6741 [==============================] - 19s - loss: 1.6016    \n",
      "Epoch 35/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 1.5113Epoch 00034: loss improved from 1.60162 to 1.51257, saving model to t4-eminem-weights-improvement-34-1.5126.hdf5\n",
      "6741/6741 [==============================] - 19s - loss: 1.5126    \n",
      "Epoch 36/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 1.4287Epoch 00035: loss improved from 1.51257 to 1.42802, saving model to t4-eminem-weights-improvement-35-1.4280.hdf5\n",
      "6741/6741 [==============================] - 18s - loss: 1.4280    \n",
      "Epoch 37/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 1.3428Epoch 00036: loss improved from 1.42802 to 1.34150, saving model to t4-eminem-weights-improvement-36-1.3415.hdf5\n",
      "6741/6741 [==============================] - 19s - loss: 1.3415    \n",
      "Epoch 38/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 1.2672Epoch 00037: loss improved from 1.34150 to 1.26672, saving model to t4-eminem-weights-improvement-37-1.2667.hdf5\n",
      "6741/6741 [==============================] - 19s - loss: 1.2667    \n",
      "Epoch 39/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 1.1808Epoch 00038: loss improved from 1.26672 to 1.18244, saving model to t4-eminem-weights-improvement-38-1.1824.hdf5\n",
      "6741/6741 [==============================] - 19s - loss: 1.1824    \n",
      "Epoch 40/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 1.1108Epoch 00039: loss improved from 1.18244 to 1.11004, saving model to t4-eminem-weights-improvement-39-1.1100.hdf5\n",
      "6741/6741 [==============================] - 18s - loss: 1.1100    \n",
      "Epoch 41/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 1.0216Epoch 00040: loss improved from 1.11004 to 1.02309, saving model to t4-eminem-weights-improvement-40-1.0231.hdf5\n",
      "6741/6741 [==============================] - 19s - loss: 1.0231    \n",
      "Epoch 42/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.9736Epoch 00041: loss improved from 1.02309 to 0.97463, saving model to t4-eminem-weights-improvement-41-0.9746.hdf5\n",
      "6741/6741 [==============================] - 18s - loss: 0.9746    \n",
      "Epoch 43/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.8938Epoch 00042: loss improved from 0.97463 to 0.89420, saving model to t4-eminem-weights-improvement-42-0.8942.hdf5\n",
      "6741/6741 [==============================] - 19s - loss: 0.8942    \n",
      "Epoch 44/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.8365Epoch 00043: loss improved from 0.89420 to 0.83667, saving model to t4-eminem-weights-improvement-43-0.8367.hdf5\n",
      "6741/6741 [==============================] - 19s - loss: 0.8367    \n",
      "Epoch 45/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.7666Epoch 00044: loss improved from 0.83667 to 0.76851, saving model to t4-eminem-weights-improvement-44-0.7685.hdf5\n",
      "6741/6741 [==============================] - 19s - loss: 0.7685    \n",
      "Epoch 46/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.7129Epoch 00045: loss improved from 0.76851 to 0.71212, saving model to t4-eminem-weights-improvement-45-0.7121.hdf5\n",
      "6741/6741 [==============================] - 18s - loss: 0.7121    \n",
      "Epoch 47/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.6576Epoch 00046: loss improved from 0.71212 to 0.65751, saving model to t4-eminem-weights-improvement-46-0.6575.hdf5\n",
      "6741/6741 [==============================] - 18s - loss: 0.6575    \n",
      "Epoch 48/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.6165Epoch 00047: loss improved from 0.65751 to 0.61860, saving model to t4-eminem-weights-improvement-47-0.6186.hdf5\n",
      "6741/6741 [==============================] - 18s - loss: 0.6186    \n",
      "Epoch 49/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.5716Epoch 00048: loss improved from 0.61860 to 0.57471, saving model to t4-eminem-weights-improvement-48-0.5747.hdf5\n",
      "6741/6741 [==============================] - 19s - loss: 0.5747    \n",
      "Epoch 50/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.5340Epoch 00049: loss improved from 0.57471 to 0.53385, saving model to t4-eminem-weights-improvement-49-0.5339.hdf5\n",
      "6741/6741 [==============================] - 18s - loss: 0.5339    \n",
      "Epoch 51/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.4895Epoch 00050: loss improved from 0.53385 to 0.49156, saving model to t4-eminem-weights-improvement-50-0.4916.hdf5\n",
      "6741/6741 [==============================] - 19s - loss: 0.4916    \n",
      "Epoch 52/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.4539Epoch 00051: loss improved from 0.49156 to 0.45533, saving model to t4-eminem-weights-improvement-51-0.4553.hdf5\n",
      "6741/6741 [==============================] - 18s - loss: 0.4553    \n",
      "Epoch 53/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.4179Epoch 00052: loss improved from 0.45533 to 0.41809, saving model to t4-eminem-weights-improvement-52-0.4181.hdf5\n",
      "6741/6741 [==============================] - 19s - loss: 0.4181    \n",
      "Epoch 54/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.3879Epoch 00053: loss improved from 0.41809 to 0.38758, saving model to t4-eminem-weights-improvement-53-0.3876.hdf5\n",
      "6741/6741 [==============================] - 18s - loss: 0.3876    \n",
      "Epoch 55/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.3513Epoch 00054: loss improved from 0.38758 to 0.35260, saving model to t4-eminem-weights-improvement-54-0.3526.hdf5\n",
      "6741/6741 [==============================] - 19s - loss: 0.3526    \n",
      "Epoch 56/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.3286Epoch 00055: loss improved from 0.35260 to 0.32837, saving model to t4-eminem-weights-improvement-55-0.3284.hdf5\n",
      "6741/6741 [==============================] - 18s - loss: 0.3284    \n",
      "Epoch 57/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.2979Epoch 00056: loss improved from 0.32837 to 0.29784, saving model to t4-eminem-weights-improvement-56-0.2978.hdf5\n",
      "6741/6741 [==============================] - 19s - loss: 0.2978    \n",
      "Epoch 58/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.2785Epoch 00057: loss improved from 0.29784 to 0.27899, saving model to t4-eminem-weights-improvement-57-0.2790.hdf5\n",
      "6741/6741 [==============================] - 19s - loss: 0.2790    \n",
      "Epoch 59/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.2615Epoch 00058: loss improved from 0.27899 to 0.26125, saving model to t4-eminem-weights-improvement-58-0.2612.hdf5\n",
      "6741/6741 [==============================] - 18s - loss: 0.2612    \n",
      "Epoch 60/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.2370Epoch 00059: loss improved from 0.26125 to 0.23765, saving model to t4-eminem-weights-improvement-59-0.2376.hdf5\n",
      "6741/6741 [==============================] - 19s - loss: 0.2376    \n",
      "Epoch 61/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.2204Epoch 00060: loss improved from 0.23765 to 0.22087, saving model to t4-eminem-weights-improvement-60-0.2209.hdf5\n",
      "6741/6741 [==============================] - 18s - loss: 0.2209    \n",
      "Epoch 62/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.2102Epoch 00061: loss improved from 0.22087 to 0.20949, saving model to t4-eminem-weights-improvement-61-0.2095.hdf5\n",
      "6741/6741 [==============================] - 19s - loss: 0.2095    \n",
      "Epoch 63/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.1995Epoch 00062: loss improved from 0.20949 to 0.20008, saving model to t4-eminem-weights-improvement-62-0.2001.hdf5\n",
      "6741/6741 [==============================] - 21s - loss: 0.2001    \n",
      "Epoch 64/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.1817Epoch 00063: loss improved from 0.20008 to 0.18155, saving model to t4-eminem-weights-improvement-63-0.1815.hdf5\n",
      "6741/6741 [==============================] - 19s - loss: 0.1815    \n",
      "Epoch 65/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.1738Epoch 00064: loss improved from 0.18155 to 0.17382, saving model to t4-eminem-weights-improvement-64-0.1738.hdf5\n",
      "6741/6741 [==============================] - 19s - loss: 0.1738    \n",
      "Epoch 66/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.1688Epoch 00065: loss improved from 0.17382 to 0.16819, saving model to t4-eminem-weights-improvement-65-0.1682.hdf5\n",
      "6741/6741 [==============================] - 19s - loss: 0.1682    \n",
      "Epoch 67/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.1574Epoch 00066: loss improved from 0.16819 to 0.15774, saving model to t4-eminem-weights-improvement-66-0.1577.hdf5\n",
      "6741/6741 [==============================] - 19s - loss: 0.1577    \n",
      "Epoch 68/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.1494Epoch 00067: loss improved from 0.15774 to 0.14983, saving model to t4-eminem-weights-improvement-67-0.1498.hdf5\n",
      "6741/6741 [==============================] - 18s - loss: 0.1498    \n",
      "Epoch 69/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.1318Epoch 00068: loss improved from 0.14983 to 0.13195, saving model to t4-eminem-weights-improvement-68-0.1319.hdf5\n",
      "6741/6741 [==============================] - 18s - loss: 0.1319    \n",
      "Epoch 70/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.1258Epoch 00069: loss improved from 0.13195 to 0.12564, saving model to t4-eminem-weights-improvement-69-0.1256.hdf5\n",
      "6741/6741 [==============================] - 19s - loss: 0.1256    \n",
      "Epoch 71/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.1253Epoch 00070: loss improved from 0.12564 to 0.12526, saving model to t4-eminem-weights-improvement-70-0.1253.hdf5\n",
      "6741/6741 [==============================] - 19s - loss: 0.1253    \n",
      "Epoch 72/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.1180Epoch 00071: loss improved from 0.12526 to 0.11769, saving model to t4-eminem-weights-improvement-71-0.1177.hdf5\n",
      "6741/6741 [==============================] - 19s - loss: 0.1177    \n",
      "Epoch 73/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.1112Epoch 00072: loss improved from 0.11769 to 0.11108, saving model to t4-eminem-weights-improvement-72-0.1111.hdf5\n",
      "6741/6741 [==============================] - 19s - loss: 0.1111    \n",
      "Epoch 74/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.1111Epoch 00073: loss did not improve\n",
      "6741/6741 [==============================] - 18s - loss: 0.1112    \n",
      "Epoch 75/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.1078Epoch 00074: loss improved from 0.11108 to 0.10743, saving model to t4-eminem-weights-improvement-74-0.1074.hdf5\n",
      "6741/6741 [==============================] - 19s - loss: 0.1074    \n",
      "Epoch 76/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.1063Epoch 00075: loss improved from 0.10743 to 0.10605, saving model to t4-eminem-weights-improvement-75-0.1061.hdf5\n",
      "6741/6741 [==============================] - 18s - loss: 0.1061    \n",
      "Epoch 77/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.0975Epoch 00076: loss improved from 0.10605 to 0.09711, saving model to t4-eminem-weights-improvement-76-0.0971.hdf5\n",
      "6741/6741 [==============================] - 19s - loss: 0.0971    \n",
      "Epoch 78/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.0941Epoch 00077: loss improved from 0.09711 to 0.09395, saving model to t4-eminem-weights-improvement-77-0.0939.hdf5\n",
      "6741/6741 [==============================] - 19s - loss: 0.0939    \n",
      "Epoch 79/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.0920Epoch 00078: loss improved from 0.09395 to 0.09188, saving model to t4-eminem-weights-improvement-78-0.0919.hdf5\n",
      "6741/6741 [==============================] - 19s - loss: 0.0919    \n",
      "Epoch 80/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.0837Epoch 00079: loss improved from 0.09188 to 0.08445, saving model to t4-eminem-weights-improvement-79-0.0845.hdf5\n",
      "6741/6741 [==============================] - 18s - loss: 0.0845    \n",
      "Epoch 81/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.0830Epoch 00080: loss improved from 0.08445 to 0.08337, saving model to t4-eminem-weights-improvement-80-0.0834.hdf5\n",
      "6741/6741 [==============================] - 19s - loss: 0.0834    \n",
      "Epoch 82/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.0749Epoch 00081: loss improved from 0.08337 to 0.07495, saving model to t4-eminem-weights-improvement-81-0.0749.hdf5\n",
      "6741/6741 [==============================] - 19s - loss: 0.0749    \n",
      "Epoch 83/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.0801Epoch 00082: loss did not improve\n",
      "6741/6741 [==============================] - 19s - loss: 0.0806    \n",
      "Epoch 84/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.0731Epoch 00083: loss improved from 0.07495 to 0.07341, saving model to t4-eminem-weights-improvement-83-0.0734.hdf5\n",
      "6741/6741 [==============================] - 18s - loss: 0.0734    \n",
      "Epoch 85/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.0664Epoch 00084: loss improved from 0.07341 to 0.06678, saving model to t4-eminem-weights-improvement-84-0.0668.hdf5\n",
      "6741/6741 [==============================] - 19s - loss: 0.0668    \n",
      "Epoch 86/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.0651Epoch 00085: loss improved from 0.06678 to 0.06516, saving model to t4-eminem-weights-improvement-85-0.0652.hdf5\n",
      "6741/6741 [==============================] - 19s - loss: 0.0652    \n",
      "Epoch 87/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.0634Epoch 00086: loss improved from 0.06516 to 0.06394, saving model to t4-eminem-weights-improvement-86-0.0639.hdf5\n",
      "6741/6741 [==============================] - 18s - loss: 0.0639    \n",
      "Epoch 88/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.0650Epoch 00087: loss did not improve\n",
      "6741/6741 [==============================] - 19s - loss: 0.0646    \n",
      "Epoch 89/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.0625Epoch 00088: loss improved from 0.06394 to 0.06243, saving model to t4-eminem-weights-improvement-88-0.0624.hdf5\n",
      "6741/6741 [==============================] - 19s - loss: 0.0624    \n",
      "Epoch 90/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.0605Epoch 00089: loss improved from 0.06243 to 0.06033, saving model to t4-eminem-weights-improvement-89-0.0603.hdf5\n",
      "6741/6741 [==============================] - 19s - loss: 0.0603    \n",
      "Epoch 91/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.0619Epoch 00090: loss did not improve\n",
      "6741/6741 [==============================] - 19s - loss: 0.0616    \n",
      "Epoch 92/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.0570Epoch 00091: loss improved from 0.06033 to 0.05673, saving model to t4-eminem-weights-improvement-91-0.0567.hdf5\n",
      "6741/6741 [==============================] - 19s - loss: 0.0567    \n",
      "Epoch 93/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.0543Epoch 00092: loss improved from 0.05673 to 0.05401, saving model to t4-eminem-weights-improvement-92-0.0540.hdf5\n",
      "6741/6741 [==============================] - 19s - loss: 0.0540    \n",
      "Epoch 94/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.0519Epoch 00093: loss improved from 0.05401 to 0.05160, saving model to t4-eminem-weights-improvement-93-0.0516.hdf5\n",
      "6741/6741 [==============================] - 19s - loss: 0.0516    \n",
      "Epoch 95/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.0574Epoch 00094: loss did not improve\n",
      "6741/6741 [==============================] - 19s - loss: 0.0573    \n",
      "Epoch 96/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.0491Epoch 00095: loss improved from 0.05160 to 0.04929, saving model to t4-eminem-weights-improvement-95-0.0493.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6741/6741 [==============================] - 19s - loss: 0.0493    \n",
      "Epoch 97/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.0521Epoch 00096: loss did not improve\n",
      "6741/6741 [==============================] - 19s - loss: 0.0524    \n",
      "Epoch 98/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.0612Epoch 00097: loss did not improve\n",
      "6741/6741 [==============================] - 19s - loss: 0.0616    \n",
      "Epoch 99/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.0670Epoch 00098: loss did not improve\n",
      "6741/6741 [==============================] - 18s - loss: 0.0669    \n",
      "Epoch 100/100\n",
      "6656/6741 [============================>.] - ETA: 0s - loss: 0.0662Epoch 00099: loss did not improve\n",
      "6741/6741 [==============================] - 18s - loss: 0.0670    \n",
      "Training took:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1912.0945937633514"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time \n",
    "# your code here\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    model.fit(X, y, epochs=100, batch_size=128,callbacks=callbacks_list)\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"Training took:\")\n",
    "t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rap Lyric Starting now:\n",
      "\" ting my sales on billboards yeah i shine like gold\n",
      "y'all clusters i'm taking over the universe y'all \"\n",
      " got them fake converses\n",
      "it's so worthless for y'all cause to verse jordeezy and youngdeezy\n",
      "cause they both go hard as shit jordeezy been dead and gone\n",
      "youngdeezy taken over his soul yeah i'm an asshole but girls love it\n",
      "im blasting shit\n",
      "bitches on my johnson no magic shit\n",
      "niggas talking crazy about my city i ain't having it\n",
      "talking about new york fell off, ain't that some shit\n",
      "if i ever see us falling off i bet im catching it\n",
      "paper on your head through a phone call im faxing shit\n",
      "oh and i get a quarter back and i sack a bitch\n",
      "adding bitches subtracting bitches that mathamatic shitt\n",
      "run up on your block and u git shocked when that static hit\n",
      "im acting up, u r l smack it up\n",
      "now u mad as fuck. you niggas softer than saggy nuts\n",
      "im stacking up this dough hotter than dragon butt\n",
      "my flow catching up you ugly couldn't bag a slut\n",
      "i was never used to\n",
      "being what i used to\n",
      "niggas carrying more than one strap so i used to\n",
      "always have to watch my own back so im used to\n",
      "talking to myself u would have thought i had a blue tooth\n",
      "y.u niggas hating...y.u niggas mad\n",
      "yo why you mad\n",
      "stop being a trashbag and get glad\n",
      "jordeezy is my comrade\n",
      "like we was in a war cuz everybody getting mad\n",
      "i dont claim to have 22 in my hand\n",
      "the only thing i have is sand\n",
      "cuz i'll put ya to sleep like the sandman\n",
      "30% rubber band man\n",
      "bombing pussy like a kamikaze\n",
      "every couple seconds i claim another victim\n",
      "is they survive then they suffer from stigmatism\n",
      "shut their whole body down like it was a system\n",
      "then tell them they had nothing to be mad or over\n",
      "next time you come after me make sure your sober\n",
      "but at the end of the day imma ask you\n",
      "y u mad son?\n",
      "uh, look\n",
      "i got richkid, deezy, and my dawg trae with me\n",
      "the only realest day one niggas with me\n",
      "i heard you talkin' about my music and how it ain't shit\n",
      "my nigga, when was the last time you made a hit?\n",
      "when was the last time, you got propped for a feature?\n",
      "i got bars to give you a sugar rush then drop with a seizure\n",
      "i got bars. hooks, flows, and even 808 tricks\n",
      "money, weed, lean, codeine and even yo' bitch\n",
      "so don't tryna ask me for any collaborations\n",
      "coz we did it already, it's just celebrations\n",
      "i'm got the colors of a zebra, i'm mixed\n",
      "you got ghost writers, your hooks are fixed\n",
      "we kill every track we're featured on\n",
      "i could even finish you on a one-on-one\n",
      "we're attached to this bitch like a pad\n",
      "yeah, but just tell me, y u mad?\n",
      "and look, i'm not tryna make you sound bad\n",
      "so my nigga, just tell me, y u mad?\n",
      "for this life i cannot change\n",
      "hidden hills, deep off in the main\n",
      "m&m’s  sweet like candy cane\n",
      "drop the top, pop it let it bang (pop it, pop it)\n",
      "for this life i cannot change\n",
      "hidden hills, deep off in the main\n",
      "m&m’s  sweet like candy cane\n",
      "drop the top, pop it let it bang (pop it, pop it)\n",
      "for this life i cannot change\n",
      "hidden hills, deep off in the main\n",
      "m&m’s  sweet like candy cane\n",
      "drop the top, pop it let it bang (pop it, pop it)\n",
      "for this life i cannot change\n",
      "hidden hills, deep off in the main\n",
      "m&m’s  sweet like candy cane\n",
      "drop the top, pop it let \n",
      " Hope it dropped some rhymes ;) \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# load text from the model\n",
    "\n",
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Rap Lyric Starting now:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "\n",
    "# generate characters\n",
    "for i in range(3000):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index) \n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print (\"\\n Hope it dropped some rhymes ;) \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
